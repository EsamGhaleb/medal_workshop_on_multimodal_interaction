{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dafbe84",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÇÔ∏è MediaPipe Pose & Holistic ‚Äî Hands‚Äëon Notebook\n",
    "Practical test‚Äëbed for MediaPipe pipelines.\n",
    "\n",
    "1. Run on **your own video** or **web‚Äëcam**.\n",
    "2. Compare **model types** and **model_complexity**.\n",
    "3. See impact of **tracking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb07cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "mp_pose, mp_holistic = mp.solutions.pose, mp.solutions.holistic\n",
    "mp_draw, mp_styles = mp.solutions.drawing_utils, mp.solutions.drawing_styles\n",
    "print('MediaPipe version:', mp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185e20c",
   "metadata": {},
   "source": [
    "## üîß Key Parameters ‚Äî MediaPipe Configuration\n",
    "\n",
    "Tune these settings to control your pipeline behavior:\n",
    "\n",
    "- **VIDEO_SOURCE**: `int` or `str`  \n",
    "   ‚Äì `0` for webcam or `'/path/to/video.mp4'`\n",
    "- **MODEL**: `{'pose', 'holistic'}`  \n",
    "   ‚Äì Choose between Pose-only or full Holistic (face, hands, pose)\n",
    "- **MODEL_COMPLEXITY**: `0 | 1 | 2`  \n",
    "   ‚Äì Trade-off between inference speed and landmark accuracy\n",
    "- **ENABLE_TRACKING**: `boolean`: `True | False`\n",
    "   ‚Äì Smooths landmarks over time (reduces jitter at the cost of slight lag)\n",
    "- **SAVE_OVERLAY**: `boolean`: `True | False`  \n",
    "   ‚Äì Write out video with drawn landmarks for later review\n",
    "\n",
    "\n",
    "| Parameter                  | Effect                    | Typical choice        | \n",
    "| -------------------------- | ------------------------- | --------------------- | \n",
    "| `model_complexity` (0/1/2) | accuracy ‚Üë, FPS ‚Üì         | 1 for live demo       | \n",
    "| `enable_tracking`          | temporal smoothing vs lag | True for lecture demo | \n",
    "| `static_image_mode`        | single frame vs stream    | False                 | \n",
    "\n",
    "Experiment with these to balance performance, accuracy, and output needs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model='holistic', model_complexity=1, enable_tracking=True):\n",
    "    if model not in {'pose', 'holistic'}: raise ValueError('model must be pose|holistic')\n",
    "    kw = dict(model_complexity=model_complexity,\n",
    "              smooth_landmarks=enable_tracking,\n",
    "              enable_segmentation=False,\n",
    "              min_detection_confidence=0.5,\n",
    "              min_tracking_confidence=0.5)\n",
    "    return (mp_pose.Pose if model=='pose' else mp_holistic.Holistic)(static_image_mode=False, **kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(source, model='holistic', model_complexity=1, enable_tracking=True,\n",
    "                  save_overlay=False, out_dir='results'):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened(): raise RuntimeError(f'Cannot open {source}')\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base = Path(str(source)).stem if isinstance(source,str) else 'webcam'\n",
    "    overlay_path = str(out_dir)+\"/\" + \"/{}_model_{}_tracking_{}_complexity_{}_overlay.mp4\".format(base, model, enable_tracking, model_complexity)\n",
    "    writer = None\n",
    "    if save_overlay:\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps=cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "        w,h=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        writer=cv2.VideoWriter(str(overlay_path),fourcc,fps,(w,h))\n",
    "    pipe=create_pipeline(model,model_complexity,enable_tracking)\n",
    "    kpts=[]\n",
    "    while True:\n",
    "        ret,frame=cap.read()\n",
    "        if not ret: break\n",
    "        res=pipe.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "        if model=='pose':\n",
    "            mp_draw.draw_landmarks(frame,res.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                   landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "        else:\n",
    "            mp_draw.draw_landmarks(frame,res.face_landmarks,mp_holistic.FACEMESH_CONTOURS,\n",
    "                                   connection_drawing_spec=mp_styles.get_default_face_mesh_contours_style())\n",
    "            mp_draw.draw_landmarks(frame,res.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_draw.draw_landmarks(frame,res.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_draw.draw_landmarks(frame,res.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                                   landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "        frame_k=[]\n",
    "        if res.pose_landmarks:\n",
    "            frame_k+=[[lm.x,lm.y,lm.z,lm.visibility] for lm in res.pose_landmarks.landmark]\n",
    "        if model=='holistic':\n",
    "            for hand in (res.left_hand_landmarks,res.right_hand_landmarks):\n",
    "                if hand:\n",
    "                    frame_k+=[[lm.x,lm.y,lm.z,1.0] for lm in hand.landmark]\n",
    "                else:\n",
    "                    frame_k+=[[0,0,0,0]]*21\n",
    "        kpts.append(frame_k)\n",
    "        if writer: writer.write(frame)\n",
    "        cv2.imshow('MediaPipe',frame)\n",
    "        if cv2.waitKey(1)==27: break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    if writer: writer.release()\n",
    "    np.save(out_dir/f'{base}_kpts.npy',np.array(kpts,dtype=np.float32))\n",
    "    return np.array(kpts), (overlay_path if save_overlay else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2005ff2",
   "metadata": {},
   "source": [
    "## üìù Exercise¬†1: Your video or webcam\n",
    "Set parameters below and run. Press **ESC** to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_SOURCE=\"input_videos/salma_hayek_short.mp4\"          # 0 for webcam or 'my_video.mp4' (specify the path to your video file)\n",
    "MODEL='holistic'        # 'pose' or 'holistic'\n",
    "MODEL_COMPLEXITY=2      # 0,1,2\n",
    "ENABLE_TRACKING=True    # smoothing\n",
    "SAVE_OVERLAY=True\n",
    "\n",
    "kpts, overlay = run_inference(VIDEO_SOURCE, MODEL, MODEL_COMPLEXITY,\n",
    "                              ENABLE_TRACKING, SAVE_OVERLAY)\n",
    "print('Keypoints shape:', kpts.shape)\n",
    "if overlay: print('Overlay saved to', overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247df02",
   "metadata": {},
   "source": [
    "## üìù Exercise 2 ‚Äî Tracking Off vs On\n",
    "\n",
    "1. Rerun Exercise 1 with  \n",
    "    ```python\n",
    "    ENABLE_TRACKING=False\n",
    "    ```\n",
    "2. Capture a short video segment (5‚Äì10 s) of moderate motion.\n",
    "3. Compare **jitter vs lag**:  \n",
    "    - Plot x-position of the nose over time for both runs on the same axes.  \n",
    "    - Compute the mean frame-to-frame Œîx and its standard deviation.  \n",
    "4. Summarize your findings:  \n",
    "    - Does smoothing reduce variance? By how much?  \n",
    "    - How much additional latency does it introduce (in ms)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558abf55",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Exercise 3 ‚Äî Landmark Indices & Trajectories\n",
    "\n",
    "1. List all pose and hand landmark indices:  \n",
    "    ```python\n",
    "    from pprint import pprint\n",
    "    pprint({i: lm.name for i, lm in enumerate(mp_pose.PoseLandmark)})\n",
    "    pprint({i: lm.name for i, lm in enumerate(mp_holistic.HandLandmark)})\n",
    "    ```\n",
    "2. Choose three landmarks (e.g., NOSE, LEFT_WRIST, RIGHT_WRIST).  \n",
    "3. Extract their 2D trajectories from `kpts` and plot over time:  \n",
    "    ```python\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # example indices\n",
    "    nose_idx = mp_pose.PoseLandmark.NOSE.value\n",
    "    lw_idx   = mp_pose.PoseLandmark.LEFT_WRIST.value\n",
    "    rw_idx   = mp_pose.PoseLandmark.RIGHT_WRIST.value\n",
    "\n",
    "    t = np.arange(kpts.shape[0])\n",
    "    for idx, label in [(nose_idx,'Nose'), (lw_idx,'L-Wrist'), (rw_idx,'R-Wrist')]:\n",
    "          x, y = kpts[:, idx, :2].T\n",
    "          plt.plot(t, x, label=f'{label} x')\n",
    "          plt.plot(t, y, '--', label=f'{label} y')\n",
    "    plt.xlabel('Frame'); plt.ylabel('Normalized coord')\n",
    "    plt.legend(); plt.show()\n",
    "    ```\n",
    "4. **Bonus**:  \n",
    "    - Identify frames where `visibility < 0.5` for each of these landmarks.  \n",
    "    - Overlay a small marker (e.g., red dot) on the video at those low-confidence frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f526ad",
   "metadata": {},
   "source": [
    "## üßê Discussion\n",
    "\n",
    "- **Model Complexity Trade-off**  \n",
    "   - Complexity 0 vs 1 vs 2: how does inference **FPS** change?  \n",
    "      ‚Ä¢ Measure end-to-end runtime on the provided clip by checking how much time it takes to process the entire video. \n",
    "   - Does higher complexity give **more accurate** landmarks?  \n",
    "      ‚Ä¢ Visually inspect overlay at key joints  \n",
    "\n",
    "- **Holistic vs Pose-Only**  \n",
    "   - **Landmark count**: Pose-only returns ~33 landmarks; Holistic adds ~468 face + 42 hands.  \n",
    "\n",
    "- **Tracking (Smoothing) On vs Off**  \n",
    "   - Jitter vs Lag:  \n",
    "      ‚Ä¢ With `ENABLE_TRACKING=True`, landmarks are smoother but react more slowly to sudden motion. This can be good if you want to track one person.   \n",
    "      ‚Ä¢ With `ENABLE_TRACKING=False`, landmarks jitter more. Does the model complexity affect this?\n",
    "   - Do you see a difference in the **stability** of landmarks?  \n",
    "\n",
    "- **Keypoints Array Structure**  \n",
    "   - Shape: `(n_frames, n_landmarks, 4)` ‚Üí `(frame, [x, y, z, visibility])`.  \n",
    "   - Visibility: **NOTE**: MediaPipe does not provide a confidence score for each landmark, but visibility indicates if the landmark is detected (1.0) or not (0.0). **This is only available for pose landmarks, not hands or face landmarks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3a63c",
   "metadata": {},
   "source": [
    "## üìù Gesture Segmentation Subset\n",
    "\n",
    "For gesture segmentation, we only need a handful of pose- and all hand-landmarks from the Holistic model. The code below already extracts these and saves them in a `.npy` file.\n",
    "\n",
    "Please run the code and check the overlay video to see how the landmarks are extracted. We already selected the best parameters for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_mp_pose import extract_keypoints\n",
    "# Path to the video you want to analyse\n",
    "video_path = \"input_videos/salma_hayek_short.mp4\"  # or specify a path to your video file\n",
    "# video_path = 0 # ‚Üê use this to use your webcam as input\n",
    "\n",
    "# Extract keypoints. The function returns a dictionary with useful metadata.\n",
    "pose_data = extract_keypoints(\n",
    "    vidf=video_path,\n",
    "    save_video=True,\n",
    "    model_complexity=MODEL_COMPLEXITY\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
