{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dafbe84",
   "metadata": {},
   "source": [
    "# üèÉ‚Äç‚ôÇÔ∏è MediaPipe Pose & Holistic ‚Äî Hands‚Äëon Notebook\n",
    "Practical test‚Äëbed for MediaPipe pipelines.\n",
    "\n",
    "1. Run on **your own video** or **web‚Äëcam**.\n",
    "2. Compare **model types** and **model_complexity**.\n",
    "3. See impact of **tracking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb07cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe version: 0.10.21\n"
     ]
    }
   ],
   "source": [
    "import os, sys, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import mediapipe as mp\n",
    "mp_pose, mp_holistic = mp.solutions.pose, mp.solutions.holistic\n",
    "mp_draw, mp_styles = mp.solutions.drawing_utils, mp.solutions.drawing_styles\n",
    "print('MediaPipe version:', mp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185e20c",
   "metadata": {},
   "source": [
    "## üîß Key Parameters ‚Äî MediaPipe Configuration\n",
    "\n",
    "Tune these settings to control your pipeline behavior:\n",
    "\n",
    "- **VIDEO_SOURCE**: `int` or `str`  \n",
    "   ‚Äì `0` for webcam or `'/path/to/video.mp4'`\n",
    "- **MODEL**: `{'pose', 'holistic'}`  \n",
    "   ‚Äì Choose between Pose-only or full Holistic (face, hands, pose)\n",
    "- **MODEL_COMPLEXITY**: `0 | 1 | 2`  \n",
    "   ‚Äì Trade-off between inference speed and landmark accuracy\n",
    "- **ENABLE_TRACKING**: `boolean`: `True | False`\n",
    "   ‚Äì Smooths landmarks over time (reduces jitter at the cost of slight lag)\n",
    "- **SAVE_OVERLAY**: `boolean`: `True | False`  \n",
    "   ‚Äì Write out video with drawn landmarks for later review\n",
    "\n",
    "Experiment with these to balance performance, accuracy, and output needs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add0e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(model='holistic', model_complexity=1, enable_tracking=True):\n",
    "    if model not in {'pose', 'holistic'}: raise ValueError('model must be pose|holistic')\n",
    "    kw = dict(model_complexity=model_complexity,\n",
    "              smooth_landmarks=enable_tracking,\n",
    "              enable_segmentation=False,\n",
    "              min_detection_confidence=0.5,\n",
    "              min_tracking_confidence=0.5)\n",
    "    return (mp_pose.Pose if model=='pose' else mp_holistic.Holistic)(static_image_mode=False, **kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a866fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(source, model='holistic', model_complexity=1, enable_tracking=True,\n",
    "                  save_overlay=False, out_dir='results'):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    if not cap.isOpened(): raise RuntimeError(f'Cannot open {source}')\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base = Path(str(source)).stem if isinstance(source,str) else 'webcam'\n",
    "    overlay_path = str(out_dir)+\"/\" + \"/{}_model_{}_tracking_{}_complexity_{}_overlay.mp4\".format(base, model, enable_tracking, model_complexity)\n",
    "    writer = None\n",
    "    if save_overlay:\n",
    "        fourcc=cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps=cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "        w,h=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        writer=cv2.VideoWriter(str(overlay_path),fourcc,fps,(w,h))\n",
    "    pipe=create_pipeline(model,model_complexity,enable_tracking)\n",
    "    kpts=[]\n",
    "    while True:\n",
    "        ret,frame=cap.read()\n",
    "        if not ret: break\n",
    "        res=pipe.process(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\n",
    "        if model=='pose':\n",
    "            mp_draw.draw_landmarks(frame,res.pose_landmarks,mp_pose.POSE_CONNECTIONS,\n",
    "                                   landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "        else:\n",
    "            mp_draw.draw_landmarks(frame,res.face_landmarks,mp_holistic.FACEMESH_CONTOURS,\n",
    "                                   connection_drawing_spec=mp_styles.get_default_face_mesh_contours_style())\n",
    "            mp_draw.draw_landmarks(frame,res.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_draw.draw_landmarks(frame,res.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_draw.draw_landmarks(frame,res.pose_landmarks,mp_holistic.POSE_CONNECTIONS,\n",
    "                                   landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "        frame_k=[]\n",
    "        if res.pose_landmarks:\n",
    "            frame_k+=[[lm.x,lm.y,lm.z,lm.visibility] for lm in res.pose_landmarks.landmark]\n",
    "        if model=='holistic':\n",
    "            for hand in (res.left_hand_landmarks,res.right_hand_landmarks):\n",
    "                if hand:\n",
    "                    frame_k+=[[lm.x,lm.y,lm.z,1.0] for lm in hand.landmark]\n",
    "                else:\n",
    "                    frame_k+=[[0,0,0,0]]*21\n",
    "        kpts.append(frame_k)\n",
    "        if writer: writer.write(frame)\n",
    "        cv2.imshow('MediaPipe',frame)\n",
    "        if cv2.waitKey(1)==27: break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    if writer: writer.release()\n",
    "    np.save(out_dir/f'{base}_kpts.npy',np.array(kpts,dtype=np.float32))\n",
    "    return np.array(kpts), (overlay_path if save_overlay else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2005ff2",
   "metadata": {},
   "source": [
    "## üìù Exercise¬†1: Your video or webcam\n",
    "Set parameters below and run. Press **ESC** to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c90d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749903076.070117 17051478 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1749903076.134921 17051882 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.160077 17051883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.161176 17051882 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.161191 17051880 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.161246 17051887 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.164668 17051884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.166000 17051883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.166345 17051882 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903076.212247 17051883 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoints shape: (27, 75, 4)\n",
      "Overlay saved to results//salma_hayek_short_model_holistic_tracking_True_complexity_2_overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "VIDEO_SOURCE=\"input_videos/salma_hayek_short.mp4\"          # 0 for webcam or 'my_video.mp4' (specify the path to your video file)\n",
    "MODEL='holistic'        # 'pose' or 'holistic'\n",
    "MODEL_COMPLEXITY=2      # 0,1,2\n",
    "ENABLE_TRACKING=True    # smoothing\n",
    "SAVE_OVERLAY=True\n",
    "\n",
    "kpts, overlay = run_inference(VIDEO_SOURCE, MODEL, MODEL_COMPLEXITY,\n",
    "                              ENABLE_TRACKING, SAVE_OVERLAY)\n",
    "print('Keypoints shape:', kpts.shape)\n",
    "if overlay: print('Overlay saved to', overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247df02",
   "metadata": {},
   "source": [
    "## üìù Exercise 2 ‚Äî Tracking Off vs On\n",
    "\n",
    "1. Rerun Exercise 1 with  \n",
    "    ```python\n",
    "    ENABLE_TRACKING=False\n",
    "    ```\n",
    "2. Capture a short video segment (5‚Äì10 s) of moderate motion.\n",
    "3. Compare **jitter vs lag**:  \n",
    "    - Plot x-position of the nose over time for both runs on the same axes.  \n",
    "    - Compute the mean frame-to-frame Œîx and its standard deviation.  \n",
    "4. Summarize your findings:  \n",
    "    - Does smoothing reduce variance? By how much?  \n",
    "    - How much additional latency does it introduce (in ms)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558abf55",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Exercise 3 ‚Äî Landmark Indices & Trajectories\n",
    "\n",
    "1. List all pose and hand landmark indices:  \n",
    "    ```python\n",
    "    from pprint import pprint\n",
    "    pprint({i: lm.name for i, lm in enumerate(mp_pose.PoseLandmark)})\n",
    "    pprint({i: lm.name for i, lm in enumerate(mp_holistic.HandLandmark)})\n",
    "    ```\n",
    "2. Choose three landmarks (e.g., NOSE, LEFT_WRIST, RIGHT_WRIST).  \n",
    "3. Extract their 2D trajectories from `kpts` and plot over time:  \n",
    "    ```python\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # example indices\n",
    "    nose_idx = mp_pose.PoseLandmark.NOSE.value\n",
    "    lw_idx   = mp_pose.PoseLandmark.LEFT_WRIST.value\n",
    "    rw_idx   = mp_pose.PoseLandmark.RIGHT_WRIST.value\n",
    "\n",
    "    t = np.arange(kpts.shape[0])\n",
    "    for idx, label in [(nose_idx,'Nose'), (lw_idx,'L-Wrist'), (rw_idx,'R-Wrist')]:\n",
    "          x, y = kpts[:, idx, :2].T\n",
    "          plt.plot(t, x, label=f'{label} x')\n",
    "          plt.plot(t, y, '--', label=f'{label} y')\n",
    "    plt.xlabel('Frame'); plt.ylabel('Normalized coord')\n",
    "    plt.legend(); plt.show()\n",
    "    ```\n",
    "4. **Bonus**:  \n",
    "    - Identify frames where `visibility < 0.5` for each of these landmarks.  \n",
    "    - Overlay a small marker (e.g., red dot) on the video at those low-confidence frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f526ad",
   "metadata": {},
   "source": [
    "## üßê Discussion\n",
    "\n",
    "- **Model Complexity Trade-off**  \n",
    "   - Complexity 0 vs 1 vs 2: how does inference **FPS** change?  \n",
    "      ‚Ä¢ Measure end-to-end runtime on the provided clip by checking how much time it takes to process the entire video. \n",
    "   - Does higher complexity give **more accurate** landmarks?  \n",
    "      ‚Ä¢ Visually inspect overlay at key joints  \n",
    "\n",
    "- **Holistic vs Pose-Only**  \n",
    "   - **Landmark count**: Pose-only returns ~33 landmarks; Holistic adds ~468 face + 42 hands.  \n",
    "\n",
    "- **Tracking (Smoothing) On vs Off**  \n",
    "   - Jitter vs Lag:  \n",
    "      ‚Ä¢ With `ENABLE_TRACKING=True`, landmarks are smoother but react more slowly to sudden motion. This can be good if you want to track one person.   \n",
    "      ‚Ä¢ With `ENABLE_TRACKING=False`, landmarks jitter more. Does the model complexity affect this?\n",
    "   - Do you see a difference in the **stability** of landmarks?  \n",
    "\n",
    "- **Keypoints Array Structure**  \n",
    "   - Shape: `(n_frames, n_landmarks, 4)` ‚Üí `(frame, [x, y, z, visibility])`.  \n",
    "   - Visibility: **NOTE**: MediaPipe does not provide a confidence score for each landmark, but visibility indicates if the landmark is detected (1.0) or not (0.0). **This is only available for pose landmarks, not hands or face landmarks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3a63c",
   "metadata": {},
   "source": [
    "## üìù Gesture Segmentation Subset\n",
    "\n",
    "For gesture segmentation, we only need a handful of pose- and all hand-landmarks from the Holistic model. The code below already extracts these and saves them in a `.npy` file.\n",
    "\n",
    "Please run the code and check the overlay video to see how the landmarks are extracted. We already selected the best parameters for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b94b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "I0000 00:00:1749903096.392733 17051478 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1920.0x1080.0, FPS: 29.97002997002997\n",
      "Number of frames in the video: 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/1136 [00:00<?, ?frame/s]W0000 00:00:1749903096.485310 17052299 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.517921 17052306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.519350 17052303 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.519378 17052297 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.519519 17052306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.523886 17052306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.523925 17052297 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749903096.525501 17052308 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1110/1136 [01:50<00:02, 10.02frame/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.extract_mp_pose import extract_keypoints\n",
    "# Path to the video you want to analyse\n",
    "video_path = \"input_videos/salma_hayek_short.mp4\"  # or specify a path to your video file\n",
    "# video_path = 0 # ‚Üê use this to use your webcam as input\n",
    "\n",
    "# Extract keypoints. The function returns a dictionary with useful metadata.\n",
    "pose_data = extract_keypoints(\n",
    "    vidf=video_path,\n",
    "    save_video=True,\n",
    "    model_complexity=MODEL_COMPLEXITY\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
