{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6172a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in required packages\n",
    "import numpy as np # package for numerical operations, especially with arrays\n",
    "import os # package that contains basic functions for inspecting folder structure etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca796d",
   "metadata": {},
   "source": [
    "## List of the landmarks provided by MediaPipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbdde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "costume_markers = ['LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW']\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_and_save_custom_landmarks(image, results, skip_pose_ids=None,\n",
    "                          hand_landmark_style=None,\n",
    "                          hand_connection_style=None,\n",
    "                          pose_point_radius=4,\n",
    "                          pose_point_color=(0,255,0),\n",
    "                          pose_connection_style=None,\n",
    "                          connect_hands_to_body=True,\n",
    "                          arm_connection_color=(255,0,0),\n",
    "                          arm_connection_thickness=2, \n",
    "                          draw=False):\n",
    "    h, w, _ = image.shape\n",
    "    \"\"\"\n",
    "    Draws all hand landmarks + filtered pose landmarks on `image`.\n",
    "\n",
    "    Args:\n",
    "      image:       BGR image to draw onto.\n",
    "      results:     Holistic.process(...) results.\n",
    "      skip_pose_ids: set of mp_holistic.PoseLandmark to omit.\n",
    "      hand_landmark_style, hand_connection_style:\n",
    "        DrawingSpec for hands (defaults to MP styles).\n",
    "      pose_point_radius, pose_point_color:\n",
    "        circle style for filtered pose points.\n",
    "      pose_connection_style:\n",
    "        DrawingSpec for pose connections (defaults to green, thickness=2).\n",
    "    \"\"\"\n",
    "    skip_pose_ids = skip_pose_ids or set()\n",
    "    # default styles\n",
    "    hand_landmark_style    = hand_landmark_style    or mp_styles.get_default_hand_landmarks_style()\n",
    "    hand_connection_style  = hand_connection_style  or mp_styles.get_default_hand_connections_style()\n",
    "    pose_connection_style  = pose_connection_style  or mp_drawing.DrawingSpec(color=pose_point_color, thickness=2)\n",
    "    facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "    # 1) draw **all** hand landmarks\n",
    "    frame_keypoints = []\n",
    "    if results.left_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add left hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no left hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add right hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no right hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    # 2) draw **filtered** pose points & connections\n",
    "    if results.pose_landmarks:\n",
    "        h, w, _ = image.shape\n",
    "        if draw:\n",
    "            # draw the points (skip any in skip_pose_ids)\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                landmark = mp_holistic.PoseLandmark(idx)\n",
    "                if landmark in skip_pose_ids:\n",
    "                    continue\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                cv2.circle(image, (x, y), pose_point_radius, pose_point_color, -1)\n",
    "        \n",
    "        # add filtered connections to frame_keypoints\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in skip_pose_ids:\n",
    "                continue\n",
    "            else:\n",
    "                frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "            \n",
    "\n",
    "        # draw connections\n",
    "        if draw:\n",
    "            # build filtered connections\n",
    "            filtered_conns = [\n",
    "                (start, end)\n",
    "                for (start, end) in mp_holistic.POSE_CONNECTIONS\n",
    "                if (mp_holistic.PoseLandmark(start) not in skip_pose_ids and\n",
    "                    mp_holistic.PoseLandmark(end  ) not in skip_pose_ids)\n",
    "            ]\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                filtered_conns,\n",
    "                landmark_drawing_spec=None,            # already drew circles\n",
    "                connection_drawing_spec=pose_connection_style\n",
    "            )\n",
    "            # 3) optionally connect each hand’s wrist back to its elbow\n",
    "            if connect_hands_to_body and results.pose_landmarks:\n",
    "                # LEFT\n",
    "                if results.left_hand_landmarks:\n",
    "                    l_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                    l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "                    p1 = (int(l_wrist.x * w), int(l_wrist.y * h))\n",
    "                    p2 = (int(l_elbow.x * w), int(l_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "\n",
    "                # RIGHT\n",
    "                if results.right_hand_landmarks:\n",
    "                    r_wrist = results.right_hand_landmarks.landmark[0]\n",
    "                    r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "                    p1 = (int(r_wrist.x * w), int(r_wrist.y * h))\n",
    "                    p2 = (int(r_elbow.x * w), int(r_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "                \n",
    "    else:   \n",
    "        # If no pose landmarks, add placeholders\n",
    "        for i in range(len(costume_markers)):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "\n",
    "    return image, frame_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef535f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.375] global cap.cpp:781 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:415: error: (-215:Assertion failed) !filename_pattern.empty() in function 'CvVideoWriter_Images'\n",
      "\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749886323.078140 16825567 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1280.0x720.0, FPS: 25.0\n",
      "Number of frames in the video: 22056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/22056 [00:00<?, ?frame/s]W0000 00:00:1749886323.148831 16825690 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.175956 16825695 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.177844 16825692 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.177843 16825699 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.177968 16825697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.181433 16825701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.181862 16825696 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886323.182822 16825690 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames:   1%|          | 115/22056 [00:04<13:49, 26.44frame/s]W0000 00:00:1749886327.443055 16825695 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing frames:   1%|▏         | 309/22056 [00:13<16:14, 22.32frame/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(18, 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Save the keypoints as npy array\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(output_path, all_kpts)\n\u001b[0;32m--> 114\u001b[0m \u001b[43mextract_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvidf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Pose_Estimation_Mediapipe_Tutorial/Scripts/input_videos/tedtalk.webm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Change this to the desired video file path\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 84\u001b[0m, in \u001b[0;36mextract_keypoints\u001b[0;34m(vidf, save_video)\u001b[0m\n\u001b[1;32m     82\u001b[0m h, w, _ \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     83\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m---> 84\u001b[0m image, kpts \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_and_save_custom_landmarks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_pose_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSKIP_POSE_IDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_point_radius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_point_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnect_hands_to_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43marm_connection_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# red lines for the “arm” link\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43marm_connection_thickness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_video:\n\u001b[1;32m     96\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(image)\n",
      "Cell \u001b[0;32mIn[3], line 131\u001b[0m, in \u001b[0;36mdraw_and_save_custom_landmarks\u001b[0;34m(image, results, skip_pose_ids, hand_landmark_style, hand_connection_style, pose_point_radius, pose_point_color, pose_connection_style, connect_hands_to_body, arm_connection_color, arm_connection_thickness, draw)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mface_landmarks:\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m draw:\n\u001b[0;32m--> 131\u001b[0m                 \u001b[43mmp_drawing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_landmarks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_landmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmp_holistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFACEMESH_TESSELATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlandmark_drawing_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmp_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_face_mesh_tesselation_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mconnection_drawing_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmp_styles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_face_mesh_contours_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# If no pose landmarks, add placeholders\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(costume_markers)):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test2/lib/python3.10/site-packages/mediapipe/python/solutions/drawing_utils.py:181\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[0;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLandmark index is out of range. Invalid connection \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    179\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom landmark #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to landmark #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m idx_to_coordinates \u001b[38;5;129;01mand\u001b[39;00m end_idx \u001b[38;5;129;01min\u001b[39;00m idx_to_coordinates:\n\u001b[0;32m--> 181\u001b[0m       drawing_spec \u001b[38;5;241m=\u001b[39m \u001b[43mconnection_drawing_spec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    182\u001b[0m           connection_drawing_spec, Mapping) \u001b[38;5;28;01melse\u001b[39;00m connection_drawing_spec\n\u001b[1;32m    183\u001b[0m       cv2\u001b[38;5;241m.\u001b[39mline(image, idx_to_coordinates[start_idx],\n\u001b[1;32m    184\u001b[0m                idx_to_coordinates[end_idx], drawing_spec\u001b[38;5;241m.\u001b[39mcolor,\n\u001b[1;32m    185\u001b[0m                drawing_spec\u001b[38;5;241m.\u001b[39mthickness)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Draws landmark points after finishing the connection lines, which is\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# aesthetically better.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: (18, 17)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Which PoseLandmark indices we want to skip because\n",
    "SKIP_POSE_IDS = {\n",
    "    mp_holistic.PoseLandmark.LEFT_WRIST,\n",
    "    mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "    mp_holistic.PoseLandmark.LEFT_PINKY,\n",
    "    mp_holistic.PoseLandmark.RIGHT_PINKY,\n",
    "    mp_holistic.PoseLandmark.LEFT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_INDEX,\n",
    "    mp_holistic.PoseLandmark.LEFT_THUMB,\n",
    "    mp_holistic.PoseLandmark.RIGHT_THUMB,\n",
    "    mp_holistic.PoseLandmark.LEFT_HIP,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HIP,\n",
    "    mp_holistic.PoseLandmark.LEFT_KNEE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_KNEE,\n",
    "    mp_holistic.PoseLandmark.LEFT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.LEFT_HEEL,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HEEL,\n",
    "    mp_holistic.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.NOSE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EAR,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EAR,\n",
    "    mp_holistic.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_holistic.PoseLandmark.MOUTH_RIGHT, \n",
    "}\n",
    "def extract_keypoints(vidf, save_video=True):    \n",
    "    \"\"\"\n",
    "    Extracts keypoints from a video file using MediaPipe Holistic.\n",
    "    Args:\n",
    "        vidf (str): Path to the video file.\n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(vidf)\n",
    "    video_name = vidf.split('/')[-1].split('.')[0]\n",
    "    video_path = os.path.dirname(vidf)\n",
    "    # save the keypoints in the same directory as the video file\n",
    "    output_path = os.path.join(video_path, video_name + '.npy')\n",
    "    # get the number of frames in the video\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Video resolution: {frameWidth}x{frameHeight}, FPS: {samplerate}\")\n",
    "    # get the number of frames in the video\n",
    "    num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # get the number of frames in the video\n",
    "    print(f\"Number of frames in the video: {num_frames}\")\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        out = cv2.VideoWriter(output_path + video_name, fourcc, \n",
    "                            fps=samplerate, frameSize=(int(frameWidth), int(frameHeight)))\n",
    "        \n",
    "    with mp_holistic.Holistic(static_image_mode=False,           # Video stream mode \n",
    "    model_complexity=2,                # Highest-accuracy pose model \n",
    "    refine_face_landmarks=False,        # Finer facial detail (iris, contours) \n",
    "    enable_segmentation=False,          # Person mask for effects\n",
    "    smooth_landmarks=True,             # Temporal smoothing to reduce jitter\n",
    "    min_detection_confidence=0.7,      # Filter weak detections\n",
    "    min_tracking_confidence=0.7        # Filter unstable tracks\n",
    "    ) as holistic:\n",
    "        all_kpts = []\n",
    "        for i in tqdm(range(num_frames), desc=\"Processing frames\", unit=\"frame\"):\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            h, w, _ = image.shape\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image, kpts = draw_and_save_custom_landmarks(\n",
    "                    image,\n",
    "                    results,\n",
    "                    skip_pose_ids=SKIP_POSE_IDS,\n",
    "                    pose_point_radius=5,\n",
    "                    pose_point_color=(0,255,0),\n",
    "                    connect_hands_to_body=True,\n",
    "                    arm_connection_color=(255,0,0),       # red lines for the “arm” link\n",
    "                    arm_connection_thickness=2,\n",
    "                    draw=True\n",
    "                )\n",
    "            if save_video:\n",
    "                out.write(image)\n",
    "            all_kpts.append(kpts)\n",
    "\n",
    "            cv2.imshow(\"merged_landmarks\", image)\n",
    "\n",
    "            if cv2.waitKey(1) == 27:\n",
    "               break\n",
    "            cv2.waitKey(1)\n",
    "        capture.release()\n",
    "        if save_video:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.destroyWindow(\"merged_landmarks\")\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    all_kpts = np.array(all_kpts)\n",
    "    # Save the keypoints as npy array\n",
    "    np.save(output_path, all_kpts)\n",
    "extract_keypoints(vidf=\"/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Pose_Estimation_Mediapipe_Tutorial/Scripts/input_videos/tedtalk.webm\", save_video=True) # Change this to the desired video file path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
