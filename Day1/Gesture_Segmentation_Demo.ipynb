{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3679256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # Python package for system-specific parameters and functions\n",
    "from test_segmentation import parse_args, set_random_seed, train_with_config, get_config \n",
    "from utils.extract_mp_pose import extract_keypoints # Extract keypoints from images using MediaPipe Pose (sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15c5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef535f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "I0000 00:00:1749886508.067567 16827188 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1280.0x720.0, FPS: 25.0\n",
      "Number of frames in the video: 22056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/22056 [00:00<?, ?frame/s]W0000 00:00:1749886508.129231 16828364 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.137828 16828362 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.138829 16828372 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.138877 16828366 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.139317 16828373 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.142770 16828373 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.143033 16828372 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749886508.143487 16828367 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames:   6%|▌         | 1353/22056 [01:03<16:11, 21.30frame/s]\n"
     ]
    }
   ],
   "source": [
    "return_dict = extract_keypoints(vidf=\"test_videos/tedtalk.webm\", save_video=True) # Change this to the desired video file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4b690c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_videos/tedtalk.npy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_dict['output_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b36e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing with poses in test_videos/tedtalk.npy ===\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing segmentation sequences...: 100%|██████████| 1/1 [00:00<00:00, 19972.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Restoring states from the checkpoint path at CABB_Segmentation/fold_1/checkpoints/fold_1/last.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/data/clusterfs/mld/users/esagha/Projects/medal_workshop_on_multimodal_interaction/Gesture_Segmentation_Toturial/Code/CABB_Segmentation/fold_1/checkpoints/CABB-segmentation-20250530_195652_fold_1' to '/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Day1/Code/CABB_Segmentation/fold_1/checkpoints/CABB-segmentation-20250614_093611_fold_1', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "Loaded model weights from the checkpoint at CABB_Segmentation/fold_1/checkpoints/fold_1/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  5  6  7  8 10] [2 4 9]\n",
      "Starting Fold 1\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fd9dce228b4b0a96ea79bb53f1312b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 11. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04947657883167267    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04947657883167267   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at CABB_Segmentation/fold_2/checkpoints/fold_2/last.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/data/clusterfs/mld/users/esagha/Projects/medal_workshop_on_multimodal_interaction/Gesture_Segmentation_Toturial/Code/CABB_Segmentation/fold_2/checkpoints/CABB-segmentation-20250530_195652_fold_2' to '/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Day1/Code/CABB_Segmentation/fold_2/checkpoints/CABB-segmentation-20250614_093611_fold_2', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "Loaded model weights from the checkpoint at CABB_Segmentation/fold_2/checkpoints/fold_2/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 7 8 9] [ 6 10]\n",
      "Starting Fold 2\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0cda9a8a2c4565b24293e7930feb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.042149174958467484    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.042149174958467484   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at CABB_Segmentation/fold_3/checkpoints/fold_3/last.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/data/clusterfs/mld/users/esagha/Projects/medal_workshop_on_multimodal_interaction/Gesture_Segmentation_Toturial/Code/CABB_Segmentation/fold_3/checkpoints/CABB-segmentation-20250530_195652_fold_3' to '/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Day1/Code/CABB_Segmentation/fold_3/checkpoints/CABB-segmentation-20250614_093611_fold_3', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "Loaded model weights from the checkpoint at CABB_Segmentation/fold_3/checkpoints/fold_3/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  6  8  9 10] [1 7]\n",
      "Starting Fold 3\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5884ed57044ae8bcb89e89cce82713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.048100654035806656    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.048100654035806656   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at CABB_Segmentation/fold_4/checkpoints/fold_4/last.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/data/clusterfs/mld/users/esagha/Projects/medal_workshop_on_multimodal_interaction/Gesture_Segmentation_Toturial/Code/CABB_Segmentation/fold_4/checkpoints/CABB-segmentation-20250530_195652_fold_4' to '/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Day1/Code/CABB_Segmentation/fold_4/checkpoints/CABB-segmentation-20250614_093611_fold_4', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "Loaded model weights from the checkpoint at CABB_Segmentation/fold_4/checkpoints/fold_4/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  4  5  6  7  9 10] [3 8]\n",
      "Starting Fold 4\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd6b5ee28b1498c81af3973f94b1b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04202672094106674    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04202672094106674   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at CABB_Segmentation/fold_5/checkpoints/fold_5/last.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:362: The dirpath has changed from '/data/clusterfs/mld/users/esagha/Projects/medal_workshop_on_multimodal_interaction/Gesture_Segmentation_Toturial/Code/CABB_Segmentation/fold_5/checkpoints/CABB-segmentation-20250530_195652_fold_5' to '/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Day1/Code/CABB_Segmentation/fold_5/checkpoints/CABB-segmentation-20250614_093611_fold_5', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "Loaded model weights from the checkpoint at CABB_Segmentation/fold_5/checkpoints/fold_5/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  6  7  8  9 10] [0 5]\n",
      "Starting Fold 5\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a2728581c441cca0b169628f9d6024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.039085280150175095    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.039085280150175095   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to CABB_Segmentation/fold_5/test_results.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pretend the notebook “argv” is your list\n",
    "sys.argv = [\"run_segmentation_test.py\"] + [\n",
    "    \"--config\",  \"config/segmentation/CABB_segment_basic_test.yaml\",  # add if you need it\n",
    "    \"--poses-path\", return_dict['output_path'],\n",
    "    \"--phase\",  \"test\",\n",
    "    \"--seed\",   \"42\",\n",
    "    \"--devices\", \"0\"\n",
    "]\n",
    "\n",
    "opts = parse_args()          # now reads from our fake sys.argv\n",
    "set_random_seed(opts.seed)\n",
    "args = get_config(opts.config)\n",
    "\n",
    "print(f\"=== Testing with poses in {opts.poses_path} ===\")\n",
    "segmentation_results = train_with_config(args, opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ea5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=return_dict['output_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae6edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 161.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 - Number of samples: 11, Number of sequences: 120\n",
      "Fold 1 - Number of samples: 11, Number of sequences: 120\n",
      "Fold 2 - Number of samples: 11, Number of sequences: 120\n",
      "Fold 3 - Number of samples: 11, Number of sequences: 120\n",
      "Fold 4 - Number of samples: 11, Number of sequences: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 115.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: 77\n",
      "Speaker: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.Gesture_Segmentation_to_ELAN import get_elan_files\n",
    "# copy segmentation_results \n",
    "segmentation_results_copy = segmentation_results.copy()\n",
    "get_elan_files(segmentation_results_copy, fps=return_dict['samplerate'], model='skeleton', threshold=0.55, file_path=return_dict['output_path'], video_output_path=return_dict['video_output_path'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
