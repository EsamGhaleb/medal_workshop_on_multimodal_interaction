{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b379bf84",
   "metadata": {},
   "source": [
    "# 🎥 Gesture Segmentation Tutorial\n",
    "This notebook demonstrates the workflow for gesture segmentation using a pre-trained model. \n",
    "It will guide you through the steps to:\n",
    "\n",
    "1. **Extract** 2‑D pose keypoints from a video using [MediaPipe Pose](https://developers.google.com/mediapipe).\n",
    "2. **Segment** the extracted skeletons with the gesture‑segmentation models.\n",
    "3. **Export** the result to ELAN for convenient manual inspection\n",
    "\n",
    "Let's start👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724757d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment initialised\n"
     ]
    }
   ],
   "source": [
    "# --- Library imports----------------------------------------------------\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Local modules\n",
    "from test_segmentation import (\n",
    "    parse_args,\n",
    "    set_random_seed,\n",
    "    train_with_config,\n",
    "    get_config,\n",
    ")\n",
    "from utils.extract_mp_pose import extract_keypoints\n",
    "\n",
    "# Add project root to PYTHONPATH \n",
    "PROJECT_ROOT = pathlib.Path.cwd()\n",
    "if PROJECT_ROOT.as_posix() not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT.as_posix())\n",
    "\n",
    "print(\"✅ Environment initialised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc25e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use these commands to make the notebook interactive and automatically reload modified modules)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8045aa",
   "metadata": {},
   "source": [
    "## 1️⃣ Extract pose keypoints\n",
    "Specify the path to **your** video file below.  \n",
    "Set `save_video=True` if you would like an overlay video with the skeleton drawn on top.\n",
    "\n",
    "**NOTE if you want to use your own webcam video**:\n",
    "- Make sure you have a webcam connected to your computer.\n",
    "- Change the `video_path` to `\"0\"` (zero) to use the webcam as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4235f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749937176.358596 17318416 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1920.0x1080.0, FPS: 29.97002997002997\n",
      "Number of frames in the video: 1136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/1136 [00:00<?, ?frame/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1749937176.454964 17318591 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.498702 17318590 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.500884 17318595 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.501023 17318594 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.501025 17318587 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.504565 17318585 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.507682 17318588 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.507822 17318596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749937176.556093 17318587 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing frames:  98%|█████████▊| 1110/1136 [02:01<00:02,  9.11frame/s]\n"
     ]
    }
   ],
   "source": [
    "# Path to the video you want to analyse\n",
    "video_path = \"input_videos/salma_hayek_short.mp4\"  # or specify a path to your video file\n",
    "# video_path = 0 # ← use this to use your webcam as input\n",
    "\n",
    "# Extract keypoints. The function returns a dictionary with useful metadata.\n",
    "pose_data = extract_keypoints(\n",
    "    vidf=video_path,\n",
    "    save_video=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22261a5",
   "metadata": {},
   "source": [
    "The dictionary contains:\n",
    "\n",
    "* `output_path` – `*.npy` file with the keypoints  \n",
    "* `video_output_path` – overlay video (optional)  \n",
    "* `samplerate` – frames‑per‑second of the processed clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47e23a",
   "metadata": {},
   "source": [
    "## 2️⃣ Run gesture segmentation\n",
    "\n",
    "The script below will load and run the gesture segmentation model on the extracted keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40df5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running segmentation on input_videos/salma_hayek_short.npy ===\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing segmentation sequences...: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Restoring states from the checkpoint path at segmentation_models/fold_1/checkpoints/fold_1/best.ckpt\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:282: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': 'val/segmentation_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\"].\n",
      "Loaded model weights from the checkpoint at segmentation_models/fold_1/checkpoints/fold_1/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4 5 6 8] [2 7]\n",
      "Starting Fold 1\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4c0399f2e642a3abae7415deb2b6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test2/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 9. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11812177300453186    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11812177300453186   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at segmentation_models/fold_2/checkpoints/fold_2/best.ckpt\n",
      "Loaded model weights from the checkpoint at segmentation_models/fold_2/checkpoints/fold_2/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 5 6 7 8] [1 4]\n",
      "Starting Fold 2\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da9d9c915c34c3a934ed6aca31b5244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11143745481967926    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11143745481967926   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at segmentation_models/fold_3/checkpoints/fold_3/best.ckpt\n",
      "Loaded model weights from the checkpoint at segmentation_models/fold_3/checkpoints/fold_3/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 7] [6 8]\n",
      "Starting Fold 3\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e061430de4284e70a727270efbbac96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10471539199352264    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10471539199352264   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at segmentation_models/fold_4/checkpoints/fold_4/best.ckpt\n",
      "Loaded model weights from the checkpoint at segmentation_models/fold_4/checkpoints/fold_4/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5 6 7 8] [0 3]\n",
      "Starting Fold 4\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42141aaa0fad4b27a4b42e72e724ae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.11593708395957947    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.11593708395957947   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at segmentation_models/fold_5/checkpoints/fold_5/best.ckpt\n",
      "Loaded model weights from the checkpoint at segmentation_models/fold_5/checkpoints/fold_5/best.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 6 7 8] [5]\n",
      "Starting Fold 5\n",
      "INFO: Trainable parameter count: 3345161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9db57171954118bdf348ad7119333f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/segmentation_loss   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10442488640546799    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/segmentation_loss  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10442488640546799   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to CABB_Segmentation/fold_5/test_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# --- Build CLI‑style arguments ---------------------------------------------\n",
    "sys.argv = [\n",
    "    \"run_segmentation_test.py\",\n",
    "    \"--config\",  \"config/segmentation/CABB_segment_basic_test.yaml\",\n",
    "    \"--poses-path\", pose_data[\"output_path\"],\n",
    "    \"--phase\",  \"test\",\n",
    "    \"--seed\",   \"42\",\n",
    "    \"--devices\", \"0\",\n",
    "    \"--models_type\", \"best\"\n",
    "]\n",
    "\n",
    "# --- Parse and run ----------------------------------------------------------\n",
    "opts = parse_args()\n",
    "set_random_seed(opts.seed)\n",
    "cfg = get_config(opts.config)\n",
    "\n",
    "print(f\"=== Running segmentation on {opts.poses_path} ===\")\n",
    "segmentation_results = train_with_config(cfg, opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc39260",
   "metadata": {},
   "source": [
    "## 3️⃣ Export to ELAN\n",
    "Convert the raw segment list into an ELAN tier.  \n",
    "Afterwards you can open the generated `.eaf` file alongside the overlay video to inspect the automatic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e0275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 184.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 - Number of samples: 9, Number of sequences: 120\n",
      "Fold 1 - Number of samples: 9, Number of sequences: 120\n",
      "Fold 2 - Number of samples: 9, Number of sequences: 120\n",
      "Fold 3 - Number of samples: 9, Number of sequences: 120\n",
      "Fold 4 - Number of samples: 9, Number of sequences: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 124.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: 77\n",
      "Speaker: B\n",
      "ELAN file saved to: input_videos/salma_hayek_short_segmentation_results_th_0.55.eaf\n",
      "✅ Finished – ELAN file ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.Gesture_Segmentation_to_ELAN import get_elan_files\n",
    "\n",
    "get_elan_files(\n",
    "    segmentation_results.copy(),\n",
    "    fps=pose_data[\"samplerate\"],\n",
    "    model=\"skeleton\",\n",
    "    threshold=0.55,\n",
    "    file_path=pose_data[\"output_path\"],\n",
    "    video_output_path=pose_data[\"video_output_path\"],\n",
    ")\n",
    "\n",
    "print(\"✅ Finished – ELAN file ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72776d35",
   "metadata": {},
   "source": [
    "## 📝 Exercise 1: Compare Model Checkpoints\n",
    "\n",
    "In Step 2️⃣ you can swap the `--models_type` argument between `\"best\"` and `\"last\"` to observe how the segmentation changes.  \n",
    "\n",
    "1. **Edit the CLI args** below (Cell 7) to select your model:\n",
    "    ```python\n",
    "    sys.argv = [\n",
    "         \"run_segmentation_test.py\",\n",
    "         \"--config\",       \"config/segmentation/CABB_segment_basic_test.yaml\",\n",
    "         \"--poses-path\",   pose_data[\"output_path\"],\n",
    "         \"--phase\",        \"test\",\n",
    "         \"--seed\",         \"42\",\n",
    "         \"--devices\",      \"0\",\n",
    "         \"--models_type\",  \"last\"    # ← try \"best\" or \"last\"\n",
    "    ]\n",
    "    ```\n",
    "2. **Rerun Cell 7** and all following cells to regenerate the ELAN file.\n",
    "3. **Open the `.eaf`** in ELAN alongside the overlay video to compare.\n",
    "\n",
    "**Discussion Questions**\n",
    "- 🔍 What differences do you notice between the `\"best\"` and `\"last\"` checkpoints?\n",
    "- 📈 Are the results substantially different?\n",
    "- 🤔 Does the accuracy of your pose keypoint extraction impact results more than the chosen model checkpoint?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cf8ee",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## 📝 Exercise 2: Compare Threshold Values\n",
    "\n",
    "The threshold value determines how confident the model must be to classify a frame as a gesture. In Step 3️⃣ you can adjust the `threshold` parameter in the `get_elan_files` call to see how it affects segmentation.\n",
    "\n",
    "1. **Edit the threshold** below in step 3️⃣ to try different values, e.g.:\n",
    "   ```python\n",
    "   get_elan_files(\n",
    "      segmentation_results.copy(),\n",
    "      fps=pose_data[\"samplerate\"],\n",
    "      model=\"skeleton\",\n",
    "      threshold=0.45,  # ← try 0.45, 0.50, 0.55, and 0.6.\n",
    "      file_path=pose_data[\"output_path\"],\n",
    "      video_output_path=pose_data[\"video_output_path\"],\n",
    "   )\n",
    "   ```\n",
    "2. **Rerun the Cell** (and any following cells) to regenerate the ELAN file.\n",
    "3. **Open the `.eaf`** in ELAN alongside the overlay video to compare how different thresholds change segment boundaries.\n",
    "\n",
    "**Discussion Questions**\n",
    "- 🔍 How does lowering or raising the threshold impact the number of detected segments?\n",
    "- 📈 Does a more permissive threshold (lower) introduce more false positives?\n",
    "- 🤔 Which threshold gives the most meaningful segmentation for your video?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
