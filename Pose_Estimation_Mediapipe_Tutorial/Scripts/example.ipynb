{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6172a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following folder is set as the output folder where all the pose time series are stored\n",
      "/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Pose_Estimation_Mediapipe_Tutorial/Scripts/Output_TimeSeries\n",
      "\n",
      " The following folder is set as the output folder for saving the masked videos \n",
      "/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Pose_Estimation_Mediapipe_Tutorial/Scripts/Output_Videos\n",
      "\n",
      " The following video(s) will be processed for masking: \n"
     ]
    }
   ],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./input_videos/\" #this is your folder with (all) your video(s)\n",
    "#time series output folder\n",
    "inputfol = \"./input_videos/\"\n",
    "outputf_mask = \"./Output_Videos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "# create output folders if they do not exist\n",
    "if not os.path.exists(outputf_mask):\n",
    "    os.makedirs(outputf_mask)\n",
    "if not os.path.exists(outtputf_ts):\n",
    "    os.makedirs(outtputf_ts)\n",
    "\n",
    "#check videos to be processed\n",
    "print(\"The following folder is set as the output folder where all the pose time series are stored\")\n",
    "print(os.path.abspath(outtputf_ts))\n",
    "print(\"\\n The following folder is set as the output folder for saving the masked videos \")\n",
    "print(os.path.abspath(outputf_mask))\n",
    "print(\"\\n The following video(s) will be processed for masking: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b240663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./input_videos/zHELICOPTER.mp4', './input_videos/zBICYCLE_FIETS.mp4']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# This will return a list of all .mp4 paths matching the pattern\n",
    "vfiles = glob.glob(\n",
    "    \"./input_videos/*.mp4\"\n",
    ")\n",
    "# remove files that end with \"overview.mp4\"\n",
    "# vfiles = [f for f in vfiles if not f.endswith(\"overview.mp4\")]\n",
    "# # keep only pair04_synced_ppB\n",
    "# vfiles = [f for f in vfiles if \"pair04_synced_ppB\" in f]\n",
    "print(vfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78a8a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_videos/zHELICOPTER.mp4', './input_videos/zBICYCLE_FIETS.mp4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7a8334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we have the following number of pose keypoints for markers body\n",
      "33\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers hands\n",
      "42\n",
      "\n",
      " Note that we have the following number of pose keypoints for markers face\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "#initialize modules and functions\n",
    "\n",
    "#load in mediapipe modules\n",
    "mp_holistic = mp.solutions.holistic\n",
    "# Import drawing_utils and drawing_styles.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    " \n",
    "##################FUNCTIONS AND OTHER VARIABLES\n",
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "costume_markers = ['LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW']\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n",
    "\n",
    "print(\"Note that we have the following number of pose keypoints for markers body\")\n",
    "print(len(markersbody))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers hands\")\n",
    "print(len(markershands))\n",
    "\n",
    "print(\"\\n Note that we have the following number of pose keypoints for markers face\")\n",
    "print(len(facemarks ))\n",
    "\n",
    "#set up the column names and objects for the time series data (add time as the first variable)\n",
    "markerxyzbody = ['time']\n",
    "markerxyzhands = ['time']\n",
    "markerxyzface = ['time']\n",
    "\n",
    "for mark in markersbody:\n",
    "    for pos in ['X', 'Y', 'Z', 'visibility']: #for markers of the body you also have a visibility reliability score\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzbody.append(nm)\n",
    "for mark in markershands:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzhands.append(nm)\n",
    "for mark in facemarks:\n",
    "    for pos in ['X', 'Y', 'Z']:\n",
    "        nm = pos + \"_\" + mark\n",
    "        markerxyzface.append(nm)\n",
    "\n",
    "#check if there are numbers in a string\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#make the stringifyd position traces into clean numerical values\n",
    "def listpostions(newsamplemarks):\n",
    "    newsamplemarks = makegoginto_str(newsamplemarks)\n",
    "    tracking_p = []\n",
    "    for value in newsamplemarks:\n",
    "        if num_there(value):\n",
    "            stripped = value.split(':', 1)[1]\n",
    "            stripped = stripped.strip() #remove spaces in the string if present\n",
    "            tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b159e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = []\n",
    "mp_to_coco_body = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "mp_to_coco_foot = [\n",
    "    31, 31, 29,   # left_big_toe, left_small_toe (dup), left_heel\n",
    "    32, 32, 30    # right_big_toe, right_small_toe (dup), right_heel\n",
    "]\n",
    "from mp2dlib import convert_landmarks_mediapipe_to_dlib\n",
    "# results.face_landmarks is the MP FaceMesh output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14648dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./input_videos/zHELICOPTER.mp4', './input_videos/zBICYCLE_FIETS.mp4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2ff49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/zHELICOPTER.mp4\n",
      "Video 1 of 2\n",
      "Processing video: ./input_videos/zBICYCLE_FIETS.mp4\n",
      "Video 2 of 2\n",
      "Done with processing all folders; go look in your output folders!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"./input_videos/./input_videos/zHELICOPTER.mp4\"\n",
      "I0000 00:00:1748277959.903393 8441312 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1748277959.961060 8450864 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.969628 8450875 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.970794 8450873 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.970940 8450875 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.971093 8450869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.974726 8450873 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.974926 8450875 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277959.976465 8450869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "OpenCV: Couldn't read video stream from file \"./input_videos/./input_videos/zBICYCLE_FIETS.mp4\"\n",
      "I0000 00:00:1748277959.995707 8441312 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "W0000 00:00:1748277960.056978 8450879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.065237 8450883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.066156 8450879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.066383 8450888 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.066391 8450884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.069876 8450879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.070211 8450888 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748277960.071717 8450884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# MASKING AND BLURRING OPTIONS \n",
    "skeleton = True\n",
    "skeleton_face_only = False         # Only show face skeleton (no body, no hands)\n",
    "whitebackground = False            # Grey background with skeleton only (no body, no face)\n",
    "maskingbody = False                # Masks the body silhouette with fully black color (original masked-piper approach)\n",
    "maskingface = False                # Masks the face (fully black color)\n",
    "blurringface = False               # Blurs the face\n",
    "blurringbody = True                # Blurs the body region\n",
    "blurringfactor = 1                 # Blurring intensity (0-1, 1 is full blur)\n",
    "# TRACE OPTIONS\n",
    "add_finger_traces = True           # Add fading traces for index fingers\n",
    "trace_length_seconds = 1.5         # Length of trace in seconds\n",
    "trace_color_left = (0, 255, 0)     # Green for left index finger trace\n",
    "trace_color_right = (0, 0, 255)    # Blue for right index finger trace\n",
    "\n",
    "# Process videos\n",
    "# MASKING AND BLURRING OPTIONS \n",
    "skeleton = True\n",
    "skeleton_face_only = False         # Only show face skeleton (no body, no hands)\n",
    "whitebackground = False            # Grey background with skeleton only (no body, no face)\n",
    "maskingbody = False                # Masks the body silhouette with fully black color (original masked-piper approach)\n",
    "maskingface = False                # Masks the face (fully black color)\n",
    "blurringface = False               # Blurs the face\n",
    "blurringbody = True                # Blurs the body region\n",
    "blurringfactor = 1                 # Blurring intensity (0-1, 1 is full blur)\n",
    "# TRACE OPTIONS\n",
    "add_finger_traces = True           # Add fading traces for index fingers\n",
    "trace_length_seconds = 2           # Length of trace in seconds\n",
    "trace_color_left = (0, 255, 0)     # Green for left index finger trace\n",
    "trace_color_right = (0, 0, 255)    # Blue for right index finger trace\n",
    "\n",
    "# Process videos\n",
    "for vidf in vfiles:\n",
    "    print(f\"Processing video: {vidf}\")\n",
    "    print(f\"Video {vfiles.index(vidf)+1} of {len(vfiles)}\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter(outputf_mask + videoname, fourcc, \n",
    "                          fps=samplerate, frameSize=(int(frameWidth), int(frameHeight)))\n",
    "    \n",
    "    time = 0\n",
    "    tsbody = [markerxyzbody]\n",
    "    tshands = [markerxyzhands]\n",
    "    tsface = [markerxyzface]\n",
    "    \n",
    "    # Initialize trace buffers for index fingers\n",
    "    if add_finger_traces:\n",
    "        trace_frames = int(trace_length_seconds * samplerate)\n",
    "        left_finger_trace = []  # Store (x, y) positions for left index finger\n",
    "        right_finger_trace = []  # Store (x, y) positions for right index finger\n",
    "    \n",
    "    with mp_holistic.Holistic(static_image_mode=False, enable_segmentation=True, refine_face_landmarks=True) as holistic:\n",
    "        while True:\n",
    "            ret, image = capture.read()\n",
    "            if ret == True:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                h, w, c = image.shape\n",
    "                \n",
    "                if np.all(results.face_landmarks) != None:\n",
    "                    # Apply white background modes first (they override other options)\n",
    "                    if whitebackground:\n",
    "                        # Create white background with only landmarks\n",
    "                        white_image = np.full((h, w, 3), (255, 255, 255), dtype=np.uint8)\n",
    "                        original_image = cv2.cvtColor(white_image, cv2.COLOR_RGB2BGR)\n",
    "                    else:\n",
    "                        # Convert to BGR for further processing\n",
    "                        original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Apply body masking if enabled (makes body fully black - original masked-piper)\n",
    "                    if maskingbody and not whitebackground:\n",
    "                        # Original masking logic\n",
    "                        image_with_alpha = np.concatenate([image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        mask_img = np.zeros_like(image, dtype=np.uint8)\n",
    "                        mask_img[:, :] = (255,255,255)\n",
    "                        segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "                        segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
    "                        annotated_image = mask_img * segm_2class * (1 - segm_2class)\n",
    "                        mask = np.concatenate([annotated_image, np.full((h, w, 1), 255, dtype=np.uint8)], axis=-1)\n",
    "                        image_with_alpha[mask==0]=0\n",
    "                        original_image = cv2.cvtColor(image_with_alpha, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    # Apply blurring to body if enabled\n",
    "                    if blurringbody and not whitebackground:\n",
    "                        # Get body mask\n",
    "                        segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
    "                        body_mask = segm_2class\n",
    "                        \n",
    "                        kernel_size = int(51 * blurringfactor)\n",
    "                        if kernel_size % 2 == 0:\n",
    "                            kernel_size += 1\n",
    "                        blurred_image = cv2.GaussianBlur(original_image, (kernel_size, kernel_size), 0)\n",
    "                        \n",
    "                        # Apply blur only to body region\n",
    "                        body_mask_3channel = cv2.merge([body_mask] * 3)\n",
    "                        original_image = (original_image * (1 - body_mask_3channel * blurringfactor) + \n",
    "                                        blurred_image * (body_mask_3channel * blurringfactor)).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply face blurring if enabled\n",
    "                    if blurringface and results.face_landmarks and not whitebackground:\n",
    "                        face_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Get all face points for a full face mask\n",
    "                        face_points = np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) \n",
    "                                              for i in range(len(landmarks))], dtype=np.int32)\n",
    "                        \n",
    "                        # Create convex hull of face points\n",
    "                        hull = cv2.convexHull(face_points)\n",
    "                        cv2.fillConvexPoly(face_mask, hull, 255)\n",
    "                        \n",
    "                        # Blur the face region\n",
    "                        kernel_size = int(51 * blurringfactor)\n",
    "                        if kernel_size % 2 == 0:\n",
    "                            kernel_size += 1\n",
    "                        blurred_image = cv2.GaussianBlur(original_image, (kernel_size, kernel_size), 0)\n",
    "                        \n",
    "                        # Apply blur only to face region\n",
    "                        face_mask_3channel = cv2.cvtColor(face_mask, cv2.COLOR_GRAY2BGR) / 255.0\n",
    "                        original_image = (original_image * (1 - face_mask_3channel * blurringfactor) + \n",
    "                                        blurred_image * (face_mask_3channel * blurringfactor)).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply face masking if enabled (makes face fully black)\n",
    "                    if maskingface and results.face_landmarks and not whitebackground:\n",
    "                        face_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "                        landmarks = results.face_landmarks.landmark\n",
    "                        \n",
    "                        # Create mask for entire face\n",
    "                        face_points = np.array([(int(landmarks[i].x * w), int(landmarks[i].y * h)) \n",
    "                                              for i in range(len(landmarks))], dtype=np.int32)\n",
    "                        \n",
    "                        # Create convex hull of face points for masking\n",
    "                        hull = cv2.convexHull(face_points)\n",
    "                        cv2.fillConvexPoly(face_mask, hull, 255)\n",
    "                        \n",
    "                        # Apply masking - make face fully black\n",
    "                        original_image[face_mask > 0] = (0, 0, 0)  # Make face fully black\n",
    "                    \n",
    "                    # Add index finger traces if enabled\n",
    "                    if add_finger_traces:\n",
    "                        # Get current index finger positions\n",
    "                        left_finger_pos = None\n",
    "                        right_finger_pos = None\n",
    "                        \n",
    "                        if results.left_hand_landmarks:\n",
    "                            # Index finger tip landmark index is 8\n",
    "                            left_landmark = results.left_hand_landmarks.landmark[8]\n",
    "                            left_finger_pos = (int(left_landmark.x * w), int(left_landmark.y * h))\n",
    "                        \n",
    "                        if results.right_hand_landmarks:\n",
    "                            # Index finger tip landmark index is 8\n",
    "                            right_landmark = results.right_hand_landmarks.landmark[8]\n",
    "                            right_finger_pos = (int(right_landmark.x * w), int(right_landmark.y * h))\n",
    "                        \n",
    "                        # Add current positions to trace buffers\n",
    "                        if left_finger_pos:\n",
    "                            left_finger_trace.append(left_finger_pos)\n",
    "                        if right_finger_pos:\n",
    "                            right_finger_trace.append(right_finger_pos)\n",
    "                        \n",
    "                        # Keep trace buffers to specified length\n",
    "                        if len(left_finger_trace) > trace_frames:\n",
    "                            left_finger_trace.pop(0)\n",
    "                        if len(right_finger_trace) > trace_frames:\n",
    "                            right_finger_trace.pop(0)\n",
    "                        \n",
    "                        # Draw fading traces for left index finger\n",
    "                        for i in range(len(left_finger_trace)-1):\n",
    "                            if i < len(left_finger_trace) and (i+1) < len(left_finger_trace):\n",
    "                                # Calculate opacity based on position in trace\n",
    "                                opacity = int(255 * (i+1) / len(left_finger_trace))\n",
    "                                alpha = opacity / 255.0\n",
    "                                \n",
    "                                # Create a copy for transparency effect\n",
    "                                overlay = original_image.copy()\n",
    "                                cv2.line(overlay, left_finger_trace[i], left_finger_trace[i+1], \n",
    "                                       trace_color_left, 2)\n",
    "                                original_image = cv2.addWeighted(overlay, alpha, original_image, 1-alpha, 0)\n",
    "                        \n",
    "                        # Draw fading traces for right index finger\n",
    "                        for i in range(len(right_finger_trace)-1):\n",
    "                            if i < len(right_finger_trace) and (i+1) < len(right_finger_trace):\n",
    "                                # Calculate opacity based on position in trace\n",
    "                                opacity = int(255 * (i+1) / len(right_finger_trace))\n",
    "                                alpha = opacity / 255.0\n",
    "                                \n",
    "                                # Create a copy for transparency effect\n",
    "                                overlay = original_image.copy()\n",
    "                                cv2.line(overlay, right_finger_trace[i], right_finger_trace[i+1], \n",
    "                                       trace_color_right, 2)\n",
    "                                original_image = cv2.addWeighted(overlay, alpha, original_image, 1-alpha, 0)\n",
    "                    \n",
    "                    # Draw landmarks conditionally\n",
    "                    if skeleton:\n",
    "                        mp_drawing.draw_landmarks(original_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        mp_drawing.draw_landmarks(original_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        \n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                        )\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.pose_landmarks,\n",
    "                            mp_holistic.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "                        )\n",
    "                    elif skeleton_face_only:\n",
    "                        # For face-only mode, draw only face landmarks\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            original_image,\n",
    "                            results.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                        )\n",
    "                    \n",
    "                    # Save time series data - FIXED for hand swapping issue\n",
    "                    samplebody = listpostions(results.pose_landmarks)\n",
    "                    sampleface = listpostions(results.face_landmarks)\n",
    "                    \n",
    "                    # Process hands separately\n",
    "                    sampleLH = listpostions(results.left_hand_landmarks)\n",
    "                    sampleRH = listpostions(results.right_hand_landmarks)\n",
    "                    \n",
    "                    # Fill empty left hand with placeholders\n",
    "                    if len(sampleLH) == 0:\n",
    "                        sampleLH = [\"\" for x in range(int(len(markerxyzhands)/2))]\n",
    "                    \n",
    "                    # Combine hands\n",
    "                    samplehands = sampleLH + sampleRH\n",
    "                    \n",
    "                    # Add time\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    \n",
    "                    # Append to time series\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                    \n",
    "                else:\n",
    "                    original_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    # Add NaN data\n",
    "                    samplebody = [np.nan for x in range(len(markerxyzbody)-1)]\n",
    "                    samplehands = [np.nan for x in range(len(markerxyzhands)-1)]\n",
    "                    sampleface = [np.nan for x in range(len(markerxyzface)-1)]\n",
    "                    samplebody.insert(0, time)\n",
    "                    samplehands.insert(0, time)\n",
    "                    sampleface.insert(0, time)\n",
    "                    tsbody.append(samplebody)\n",
    "                    tshands.append(samplehands)\n",
    "                    tsface.append(sampleface)\n",
    "                \n",
    "                cv2.imshow(\"resizedimage\", original_image)\n",
    "                out.write(original_image)\n",
    "                time = time + (1000/samplerate)\n",
    "                \n",
    "            if cv2.waitKey(1) == 27:\n",
    "                break\n",
    "            if ret == False:\n",
    "                break\n",
    "    \n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # # Save CSV files\n",
    "    # filebody = open(outtputf_ts + vidf[:-4] + '_body.csv', 'w+', newline='')\n",
    "    # with filebody:\n",
    "    #     write = csv.writer(filebody)\n",
    "    #     write.writerows(tsbody)\n",
    "    \n",
    "    # filehands = open(outtputf_ts + vidf[:-4] + '_hands.csv', 'w+', newline='')\n",
    "    # with filehands:\n",
    "    #     write = csv.writer(filehands)\n",
    "    #     write.writerows(tshands)\n",
    "    \n",
    "    # fileface = open(outtputf_ts + vidf[:-4] + '_face.csv', 'w+', newline='')\n",
    "    # with fileface:\n",
    "    #     write = csv.writer(fileface)\n",
    "    #     write.writerows(tsface)\n",
    "\n",
    "print(\"Done with processing all folders; go look in your output folders!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_and_save_custom_landmarks(image, results, skip_pose_ids=None,\n",
    "                          hand_landmark_style=None,\n",
    "                          hand_connection_style=None,\n",
    "                          pose_point_radius=4,\n",
    "                          pose_point_color=(0,255,0),\n",
    "                          pose_connection_style=None,\n",
    "                          connect_hands_to_body=True,\n",
    "                          arm_connection_color=(255,0,0),\n",
    "                          arm_connection_thickness=2, \n",
    "                          draw=False):\n",
    "    h, w, _ = image.shape\n",
    "    \"\"\"\n",
    "    Draws all hand landmarks + filtered pose landmarks on `image`.\n",
    "\n",
    "    Args:\n",
    "      image:       BGR image to draw onto.\n",
    "      results:     Holistic.process(...) results.\n",
    "      skip_pose_ids: set of mp_holistic.PoseLandmark to omit.\n",
    "      hand_landmark_style, hand_connection_style:\n",
    "        DrawingSpec for hands (defaults to MP styles).\n",
    "      pose_point_radius, pose_point_color:\n",
    "        circle style for filtered pose points.\n",
    "      pose_connection_style:\n",
    "        DrawingSpec for pose connections (defaults to green, thickness=2).\n",
    "    \"\"\"\n",
    "    skip_pose_ids = skip_pose_ids or set()\n",
    "    # default styles\n",
    "    hand_landmark_style    = hand_landmark_style    or mp_styles.get_default_hand_landmarks_style()\n",
    "    hand_connection_style  = hand_connection_style  or mp_styles.get_default_hand_connections_style()\n",
    "    pose_connection_style  = pose_connection_style  or mp_drawing.DrawingSpec(color=pose_point_color, thickness=2)\n",
    "\n",
    "    # 1) draw **all** hand landmarks\n",
    "    frame_keypoints = []\n",
    "    if results.left_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add left hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no left hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add right hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no right hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    # 2) draw **filtered** pose points & connections\n",
    "    if results.pose_landmarks:\n",
    "        h, w, _ = image.shape\n",
    "        if draw:\n",
    "            # draw the points (skip any in skip_pose_ids)\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                landmark = mp_holistic.PoseLandmark(idx)\n",
    "                if landmark in skip_pose_ids:\n",
    "                    continue\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                cv2.circle(image, (x, y), pose_point_radius, pose_point_color, -1)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # add filtered connections to frame_keypoints\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in skip_pose_ids:\n",
    "                continue\n",
    "            else:\n",
    "                frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "            \n",
    "\n",
    "        # draw connections\n",
    "        if draw:\n",
    "            # build filtered connections\n",
    "            filtered_conns = [\n",
    "                (start, end)\n",
    "                for (start, end) in mp_holistic.POSE_CONNECTIONS\n",
    "                if (mp_holistic.PoseLandmark(start) not in skip_pose_ids and\n",
    "                    mp_holistic.PoseLandmark(end  ) not in skip_pose_ids)\n",
    "            ]\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                filtered_conns,\n",
    "                landmark_drawing_spec=None,            # already drew circles\n",
    "                connection_drawing_spec=pose_connection_style\n",
    "            )\n",
    "            # 3) optionally connect each hand’s wrist back to its elbow\n",
    "            if connect_hands_to_body and results.pose_landmarks:\n",
    "                # LEFT\n",
    "                if results.left_hand_landmarks:\n",
    "                    l_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                    l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "                    p1 = (int(l_wrist.x * w), int(l_wrist.y * h))\n",
    "                    p2 = (int(l_elbow.x * w), int(l_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "\n",
    "                # RIGHT\n",
    "                if results.right_hand_landmarks:\n",
    "                    r_wrist = results.right_hand_landmarks.landmark[0]\n",
    "                    r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "                    p1 = (int(r_wrist.x * w), int(r_wrist.y * h))\n",
    "                    p2 = (int(r_elbow.x * w), int(r_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "    else:   \n",
    "        # If no pose landmarks, add placeholders\n",
    "        for i in range(len(costume_markers)):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "\n",
    "    return image, frame_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef535f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748281750.190886 8441312 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/zHELICOPTER.mp4\n",
      "Video 1 of 2\n",
      "Number of frames in the video: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 0frame [00:00, ?frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/zBICYCLE_FIETS.mp4\n",
      "Video 2 of 2\n",
      "Number of frames in the video: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "W0000 00:00:1748281750.252812 8490702 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.261634 8490703 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.262815 8490710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.262826 8490706 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.262840 8490702 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.266227 8490702 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.266633 8490705 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.266765 8490710 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1748281750.317886 8441312 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "Processing frames: 0frame [00:00, ?frame/s]\n",
      "W0000 00:00:1748281750.373942 8490723 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.382475 8490726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.383676 8490726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.383818 8490730 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.383827 8490727 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.387178 8490731 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.387881 8490726 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748281750.387939 8490724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Which PoseLandmark indices we want to skip because\n",
    "SKIP_POSE_IDS = {\n",
    "    mp_holistic.PoseLandmark.LEFT_WRIST,\n",
    "    mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "    mp_holistic.PoseLandmark.LEFT_PINKY,\n",
    "    mp_holistic.PoseLandmark.RIGHT_PINKY,\n",
    "    mp_holistic.PoseLandmark.LEFT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_INDEX,\n",
    "    mp_holistic.PoseLandmark.LEFT_THUMB,\n",
    "    mp_holistic.PoseLandmark.RIGHT_THUMB,\n",
    "    mp_holistic.PoseLandmark.LEFT_HIP,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HIP,\n",
    "    mp_holistic.PoseLandmark.LEFT_KNEE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_KNEE,\n",
    "    mp_holistic.PoseLandmark.LEFT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.LEFT_HEEL,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HEEL,\n",
    "    mp_holistic.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.NOSE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EAR,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EAR,\n",
    "    mp_holistic.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_holistic.PoseLandmark.MOUTH_RIGHT, \n",
    "}\n",
    "# Process videos\n",
    "for vidf in vfiles:\n",
    "    print(f\"Processing video: {vidf}\")\n",
    "    print(f\"Video {vfiles.index(vidf)+1} of {len(vfiles)}\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = inputfol + videoname\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    # get the number of frames in the video\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    # get the number of frames in the video\n",
    "    num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # get the number of frames in the video\n",
    "    print(f\"Number of frames in the video: {num_frames}\")\n",
    "    with mp_holistic.Holistic(static_image_mode=False,           # Video stream mode :contentReference[oaicite:7]{index=7}\n",
    "    model_complexity=1,                # Highest-accuracy pose model :contentReference[oaicite:8]{index=8}\n",
    "    refine_face_landmarks=False,        # Finer facial detail (iris, contours) :contentReference[oaicite:9]{index=9}\n",
    "    enable_segmentation=False,          # Person mask for effects :contentReference[oaicite:10]{index=10}\n",
    "    smooth_landmarks=True,             # Temporal smoothing to reduce jitter :contentReference[oaicite:11]{index=11}\n",
    "    min_detection_confidence=0.7,      # Filter weak detections :contentReference[oaicite:12]{index=12}\n",
    "    min_tracking_confidence=0.7        # Filter unstable tracks :contentReference[oaicite:13]{index=13}\n",
    "    ) as holistic:\n",
    "        all_kpts = []\n",
    "        for i in tqdm(range(num_frames), desc=\"Processing frames\", unit=\"frame\"):\n",
    "            \n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            h, w, _ = image.shape\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image, kpts = draw_and_save_custom_landmarks(\n",
    "                    image,\n",
    "                    results,\n",
    "                    skip_pose_ids=SKIP_POSE_IDS,\n",
    "                    pose_point_radius=5,\n",
    "                    pose_point_color=(0,255,0),\n",
    "                    connect_hands_to_body=True,\n",
    "                    arm_connection_color=(255,0,0),       # red lines for the “arm” link\n",
    "                    arm_connection_thickness=2\n",
    "                )\n",
    "            all_kpts.append(kpts)\n",
    "            if cv2.waitKey(1) == 27:\n",
    "               break\n",
    "            cv2.imshow(\"merged_landmarks\", image)\n",
    "            cv2.waitKey(1)\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    all_kpts = np.array(all_kpts)\n",
    "    # Save the keypoints as npy array\n",
    "    video_name = vidf.split('/')[-1].split('.')[0]\n",
    "    np.save(outtputf_ts + video_name+ '_all_kpts_17.npy', all_kpts)\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "265dd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# # This will return a list of all .mp4 paths matching the pattern\n",
    "# file_paths = glob.glob(\n",
    "#     \"/Users/esamghaleb/Documents/ResearchData/CABB Small Dataset/processed_audio_video/*/*.mp4\"\n",
    "# )\n",
    "\n",
    "# print(file_paths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
