{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6172a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in required packages\n",
    "import numpy as np # package for numerical operations, especially with arrays\n",
    "import os # package that contains basic functions for inspecting folder structure etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca796d",
   "metadata": {},
   "source": [
    "## List of the landmarks provided by MediaPipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bbdde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#landmarks 33x that are used by Mediapipe (Blazepose)\n",
    "markersbody = ['NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER', 'RIGHT_EYE_OUTER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
    "          'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT', 'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW', 'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY', 'LEFT_INDEX', 'RIGHT_INDEX',\n",
    "          'LEFT_THUMB', 'RIGHT_THUMB', 'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE', 'LEFT_ANKLE', 'RIGHT_ANKLE',\n",
    "          'LEFT_HEEL', 'RIGHT_HEEL', 'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX']\n",
    "costume_markers = ['LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', \n",
    "          'RIGHT_ELBOW']\n",
    "markershands = ['LEFT_WRIST', 'LEFT_THUMB_CMC', 'LEFT_THUMB_MCP', 'LEFT_THUMB_IP', 'LEFT_THUMB_TIP', 'LEFT_INDEX_FINGER_MCP',\n",
    "              'LEFT_INDEX_FINGER_PIP', 'LEFT_INDEX_FINGER_DIP', 'LEFT_INDEX_FINGER_TIP', 'LEFT_MIDDLE_FINGER_MCP', \n",
    "               'LEFT_MIDDLE_FINGER_PIP', 'LEFT_MIDDLE_FINGER_DIP', 'LEFT_MIDDLE_FINGER_TIP', 'LEFT_RING_FINGER_MCP', \n",
    "               'LEFT_RING_FINGER_PIP', 'LEFT_RING_FINGER_DIP', 'LEFT_RING_FINGER_TIP', 'LEFT_PINKY_FINGER_MCP', \n",
    "               'LEFT_PINKY_FINGER_PIP', 'LEFT_PINKY_FINGER_DIP', 'LEFT_PINKY_FINGER_TIP',\n",
    "              'RIGHT_WRIST', 'RIGHT_THUMB_CMC', 'RIGHT_THUMB_MCP', 'RIGHT_THUMB_IP', 'RIGHT_THUMB_TIP', 'RIGHT_INDEX_FINGER_MCP',\n",
    "              'RIGHT_INDEX_FINGER_PIP', 'RIGHT_INDEX_FINGER_DIP', 'RIGHT_INDEX_FINGER_TIP', 'RIGHT_MIDDLE_FINGER_MCP', \n",
    "               'RIGHT_MIDDLE_FINGER_PIP', 'RIGHT_MIDDLE_FINGER_DIP', 'RIGHT_MIDDLE_FINGER_TIP', 'RIGHT_RING_FINGER_MCP', \n",
    "               'RIGHT_RING_FINGER_PIP', 'RIGHT_RING_FINGER_DIP', 'RIGHT_RING_FINGER_TIP', 'RIGHT_PINKY_FINGER_MCP', \n",
    "               'RIGHT_PINKY_FINGER_PIP', 'RIGHT_PINKY_FINGER_DIP', 'RIGHT_PINKY_FINGER_TIP']\n",
    "facemarks = [str(x) for x in range(478)] #there are 478 points for the face mesh (see google holistic face mesh info for landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_and_save_custom_landmarks(image, results, skip_pose_ids=None,\n",
    "                          hand_landmark_style=None,\n",
    "                          hand_connection_style=None,\n",
    "                          pose_point_radius=4,\n",
    "                          pose_point_color=(0,255,0),\n",
    "                          pose_connection_style=None,\n",
    "                          connect_hands_to_body=True,\n",
    "                          arm_connection_color=(255,0,0),\n",
    "                          arm_connection_thickness=2, \n",
    "                          draw=False):\n",
    "    h, w, _ = image.shape\n",
    "    \"\"\"\n",
    "    Draws all hand landmarks + filtered pose landmarks on `image`.\n",
    "\n",
    "    Args:\n",
    "      image:       BGR image to draw onto.\n",
    "      results:     Holistic.process(...) results.\n",
    "      skip_pose_ids: set of mp_holistic.PoseLandmark to omit.\n",
    "      hand_landmark_style, hand_connection_style:\n",
    "        DrawingSpec for hands (defaults to MP styles).\n",
    "      pose_point_radius, pose_point_color:\n",
    "        circle style for filtered pose points.\n",
    "      pose_connection_style:\n",
    "        DrawingSpec for pose connections (defaults to green, thickness=2).\n",
    "    \"\"\"\n",
    "    skip_pose_ids = skip_pose_ids or set()\n",
    "    # default styles\n",
    "    hand_landmark_style    = hand_landmark_style    or mp_styles.get_default_hand_landmarks_style()\n",
    "    hand_connection_style  = hand_connection_style  or mp_styles.get_default_hand_connections_style()\n",
    "    pose_connection_style  = pose_connection_style  or mp_drawing.DrawingSpec(color=pose_point_color, thickness=2)\n",
    "\n",
    "    # 1) draw **all** hand landmarks\n",
    "    frame_keypoints = []\n",
    "    if results.left_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add left hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no left hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add right hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no right hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    # 2) draw **filtered** pose points & connections\n",
    "    if results.pose_landmarks:\n",
    "        h, w, _ = image.shape\n",
    "        if draw:\n",
    "            # draw the points (skip any in skip_pose_ids)\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                landmark = mp_holistic.PoseLandmark(idx)\n",
    "                if landmark in skip_pose_ids:\n",
    "                    continue\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                cv2.circle(image, (x, y), pose_point_radius, pose_point_color, -1)\n",
    "        \n",
    "        # add filtered connections to frame_keypoints\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in skip_pose_ids:\n",
    "                continue\n",
    "            else:\n",
    "                frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "            \n",
    "\n",
    "        # draw connections\n",
    "        if draw:\n",
    "            # build filtered connections\n",
    "            filtered_conns = [\n",
    "                (start, end)\n",
    "                for (start, end) in mp_holistic.POSE_CONNECTIONS\n",
    "                if (mp_holistic.PoseLandmark(start) not in skip_pose_ids and\n",
    "                    mp_holistic.PoseLandmark(end  ) not in skip_pose_ids)\n",
    "            ]\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                filtered_conns,\n",
    "                landmark_drawing_spec=None,            # already drew circles\n",
    "                connection_drawing_spec=pose_connection_style\n",
    "            )\n",
    "            # 3) optionally connect each hand’s wrist back to its elbow\n",
    "            if connect_hands_to_body and results.pose_landmarks:\n",
    "                # LEFT\n",
    "                if results.left_hand_landmarks:\n",
    "                    l_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                    l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "                    p1 = (int(l_wrist.x * w), int(l_wrist.y * h))\n",
    "                    p2 = (int(l_elbow.x * w), int(l_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "\n",
    "                # RIGHT\n",
    "                if results.right_hand_landmarks:\n",
    "                    r_wrist = results.right_hand_landmarks.landmark[0]\n",
    "                    r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "                    p1 = (int(r_wrist.x * w), int(r_wrist.y * h))\n",
    "                    p2 = (int(r_elbow.x * w), int(r_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "    else:   \n",
    "        # If no pose landmarks, add placeholders\n",
    "        for i in range(len(costume_markers)):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "\n",
    "    return image, frame_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef535f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1280.0x720.0, FPS: 25.0\n",
      "Number of frames in the video: 22056\n",
      "Downloading model to /opt/anaconda3/envs/test2/lib/python3.10/site-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2.892] global cap.cpp:781 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:415: error: (-215:Assertion failed) !filename_pattern.empty() in function 'CvVideoWriter_Images'\n",
      "\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749846838.813973 16627269 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1749846838.878274 16627586 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.905545 16627586 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.908100 16627590 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.908120 16627584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.908185 16627588 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.911402 16627588 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.913072 16627586 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749846838.913133 16627584 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames:   1%|          | 115/22056 [00:04<13:03, 28.02frame/s]W0000 00:00:1749846843.523311 16627590 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing frames:   2%|▏         | 355/22056 [00:18<18:28, 19.58frame/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Which PoseLandmark indices we want to skip because\n",
    "SKIP_POSE_IDS = {\n",
    "    mp_holistic.PoseLandmark.LEFT_WRIST,\n",
    "    mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "    mp_holistic.PoseLandmark.LEFT_PINKY,\n",
    "    mp_holistic.PoseLandmark.RIGHT_PINKY,\n",
    "    mp_holistic.PoseLandmark.LEFT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_INDEX,\n",
    "    mp_holistic.PoseLandmark.LEFT_THUMB,\n",
    "    mp_holistic.PoseLandmark.RIGHT_THUMB,\n",
    "    mp_holistic.PoseLandmark.LEFT_HIP,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HIP,\n",
    "    mp_holistic.PoseLandmark.LEFT_KNEE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_KNEE,\n",
    "    mp_holistic.PoseLandmark.LEFT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.LEFT_HEEL,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HEEL,\n",
    "    mp_holistic.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.NOSE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EAR,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EAR,\n",
    "    mp_holistic.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_holistic.PoseLandmark.MOUTH_RIGHT, \n",
    "}\n",
    "def extract_keypoints(vidf, save_video=True):    \n",
    "    \"\"\"\n",
    "    Extracts keypoints from a video file using MediaPipe Holistic.\n",
    "    Args:\n",
    "        vidf (str): Path to the video file.\n",
    "    \"\"\"\n",
    "    capture = cv2.VideoCapture(vidf)\n",
    "    video_name = vidf.split('/')[-1].split('.')[0]\n",
    "    video_path = os.path.dirname(vidf)\n",
    "    # save the keypoints in the same directory as the video file\n",
    "    output_path = os.path.join(video_path, video_name + '.npy')\n",
    "    # get the number of frames in the video\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Video resolution: {frameWidth}x{frameHeight}, FPS: {samplerate}\")\n",
    "    # get the number of frames in the video\n",
    "    num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # get the number of frames in the video\n",
    "    print(f\"Number of frames in the video: {num_frames}\")\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        out = cv2.VideoWriter(output_path + video_name, fourcc, \n",
    "                            fps=samplerate, frameSize=(int(frameWidth), int(frameHeight)))\n",
    "        \n",
    "    with mp_holistic.Holistic(static_image_mode=False,           # Video stream mode \n",
    "    model_complexity=2,                # Highest-accuracy pose model \n",
    "    refine_face_landmarks=False,        # Finer facial detail (iris, contours) \n",
    "    enable_segmentation=False,          # Person mask for effects\n",
    "    smooth_landmarks=True,             # Temporal smoothing to reduce jitter\n",
    "    min_detection_confidence=0.7,      # Filter weak detections\n",
    "    min_tracking_confidence=0.7        # Filter unstable tracks\n",
    "    ) as holistic:\n",
    "        all_kpts = []\n",
    "        for i in tqdm(range(num_frames), desc=\"Processing frames\", unit=\"frame\"):\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            h, w, _ = image.shape\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image, kpts = draw_and_save_custom_landmarks(\n",
    "                    image,\n",
    "                    results,\n",
    "                    skip_pose_ids=SKIP_POSE_IDS,\n",
    "                    pose_point_radius=5,\n",
    "                    pose_point_color=(0,255,0),\n",
    "                    connect_hands_to_body=True,\n",
    "                    arm_connection_color=(255,0,0),       # red lines for the “arm” link\n",
    "                    arm_connection_thickness=2,\n",
    "                    draw=True\n",
    "                )\n",
    "            if save_video:\n",
    "                out.write(image)\n",
    "            all_kpts.append(kpts)\n",
    "\n",
    "            cv2.imshow(\"merged_landmarks\", image)\n",
    "\n",
    "            if cv2.waitKey(1) == 27:\n",
    "               break\n",
    "            cv2.waitKey(1)\n",
    "        capture.release()\n",
    "        if save_video:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.destroyWindow(\"merged_landmarks\")\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    all_kpts = np.array(all_kpts)\n",
    "    # Save the keypoints as npy array\n",
    "    np.save(output_path, all_kpts)\n",
    "extract_keypoints(vidf=\"/Users/esagha/Projects/medal_workshop_on_multimodal_interaction/Pose_Estimation_Mediapipe_Tutorial/Scripts/input_videos/tedtalk.webm\", save_video=True) # Change this to the desired video file path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
