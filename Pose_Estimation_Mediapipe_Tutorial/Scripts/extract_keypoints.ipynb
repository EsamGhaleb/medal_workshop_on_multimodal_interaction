{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6172a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in required packages\n",
    "import mediapipe as mp #mediapipe\n",
    "import cv2 #opencv\n",
    "import math #basic operations\n",
    "import numpy as np #basic operations\n",
    "import pandas as pd #data wrangling\n",
    "import csv #csv saving\n",
    "import os #some basic functions for inspecting folder structure etc.\n",
    "\n",
    "#list all videos in input_videofolder\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"./input_videos/\" #this is your folder with (all) your video(s)\n",
    "#time series output folder\n",
    "inputfol = \"./input_videos/\"\n",
    "outputf_mask = \"./Output_Videos/\"\n",
    "outtputf_ts = \"./Output_TimeSeries/\"\n",
    "# create output folders if they do not exist\n",
    "if not os.path.exists(outputf_mask):\n",
    "    os.makedirs(outputf_mask)\n",
    "if not os.path.exists(outtputf_ts):\n",
    "    os.makedirs(outtputf_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b240663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./input_videos/zHELICOPTER.mp4', './input_videos/zBICYCLE_FIETS.mp4', './input_videos/S12B.mp4']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# This will return a list of all .mp4 paths matching the pattern\n",
    "vfiles = glob.glob(\n",
    "    \"./input_videos/*.mp4\"\n",
    ")\n",
    "print(vfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b159e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = []\n",
    "mp_to_coco_body = [0, 2, 5, 7, 8, 11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28]\n",
    "mp_to_coco_foot = [\n",
    "    31, 31, 29,   # left_big_toe, left_small_toe (dup), left_heel\n",
    "    32, 32, 30    # right_big_toe, right_small_toe (dup), right_heel\n",
    "]\n",
    "from mp2dlib import convert_landmarks_mediapipe_to_dlib\n",
    "# results.face_landmarks is the MP FaceMesh output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc0f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def draw_and_save_custom_landmarks(image, results, skip_pose_ids=None,\n",
    "                          hand_landmark_style=None,\n",
    "                          hand_connection_style=None,\n",
    "                          pose_point_radius=4,\n",
    "                          pose_point_color=(0,255,0),\n",
    "                          pose_connection_style=None,\n",
    "                          connect_hands_to_body=True,\n",
    "                          arm_connection_color=(255,0,0),\n",
    "                          arm_connection_thickness=2, \n",
    "                          draw=False):\n",
    "    h, w, _ = image.shape\n",
    "    \"\"\"\n",
    "    Draws all hand landmarks + filtered pose landmarks on `image`.\n",
    "\n",
    "    Args:\n",
    "      image:       BGR image to draw onto.\n",
    "      results:     Holistic.process(...) results.\n",
    "      skip_pose_ids: set of mp_holistic.PoseLandmark to omit.\n",
    "      hand_landmark_style, hand_connection_style:\n",
    "        DrawingSpec for hands (defaults to MP styles).\n",
    "      pose_point_radius, pose_point_color:\n",
    "        circle style for filtered pose points.\n",
    "      pose_connection_style:\n",
    "        DrawingSpec for pose connections (defaults to green, thickness=2).\n",
    "    \"\"\"\n",
    "    skip_pose_ids = skip_pose_ids or set()\n",
    "    # default styles\n",
    "    hand_landmark_style    = hand_landmark_style    or mp_styles.get_default_hand_landmarks_style()\n",
    "    hand_connection_style  = hand_connection_style  or mp_styles.get_default_hand_connections_style()\n",
    "    pose_connection_style  = pose_connection_style  or mp_drawing.DrawingSpec(color=pose_point_color, thickness=2)\n",
    "\n",
    "    # 1) draw **all** hand landmarks\n",
    "    frame_keypoints = []\n",
    "    if results.left_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add left hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.left_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no left hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        if draw:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=hand_landmark_style,\n",
    "                connection_drawing_spec=hand_connection_style\n",
    "            )\n",
    "        # add right hand keypoints to frame_keypoints\n",
    "        for idx, lm in enumerate(results.right_hand_landmarks.landmark):\n",
    "            frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    else:\n",
    "        # If no right hand landmarks, add placeholders\n",
    "        for i in range(21):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "    # 2) draw **filtered** pose points & connections\n",
    "    if results.pose_landmarks:\n",
    "        h, w, _ = image.shape\n",
    "        if draw:\n",
    "            # draw the points (skip any in skip_pose_ids)\n",
    "            for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "                landmark = mp_holistic.PoseLandmark(idx)\n",
    "                if landmark in skip_pose_ids:\n",
    "                    continue\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                cv2.circle(image, (x, y), pose_point_radius, pose_point_color, -1)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # add filtered connections to frame_keypoints\n",
    "        for idx, lm in enumerate(results.pose_landmarks.landmark):\n",
    "            if idx in skip_pose_ids:\n",
    "                continue\n",
    "            else:\n",
    "                frame_keypoints.append([lm.x, lm.y, lm.z, lm.visibility])\n",
    "            \n",
    "\n",
    "        # draw connections\n",
    "        if draw:\n",
    "            # build filtered connections\n",
    "            filtered_conns = [\n",
    "                (start, end)\n",
    "                for (start, end) in mp_holistic.POSE_CONNECTIONS\n",
    "                if (mp_holistic.PoseLandmark(start) not in skip_pose_ids and\n",
    "                    mp_holistic.PoseLandmark(end  ) not in skip_pose_ids)\n",
    "            ]\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                filtered_conns,\n",
    "                landmark_drawing_spec=None,            # already drew circles\n",
    "                connection_drawing_spec=pose_connection_style\n",
    "            )\n",
    "            # 3) optionally connect each hand’s wrist back to its elbow\n",
    "            if connect_hands_to_body and results.pose_landmarks:\n",
    "                # LEFT\n",
    "                if results.left_hand_landmarks:\n",
    "                    l_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                    l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "                    p1 = (int(l_wrist.x * w), int(l_wrist.y * h))\n",
    "                    p2 = (int(l_elbow.x * w), int(l_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "\n",
    "                # RIGHT\n",
    "                if results.right_hand_landmarks:\n",
    "                    r_wrist = results.right_hand_landmarks.landmark[0]\n",
    "                    r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "                    p1 = (int(r_wrist.x * w), int(r_wrist.y * h))\n",
    "                    p2 = (int(r_elbow.x * w), int(r_elbow.y * h))\n",
    "                    cv2.line(image, p1, p2, arm_connection_color, arm_connection_thickness)\n",
    "    else:   \n",
    "        # If no pose landmarks, add placeholders\n",
    "        for i in range(len(costume_markers)):\n",
    "            frame_keypoints.append([0, 0, 0, 0])\n",
    "\n",
    "    return image, frame_keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef535f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749735315.799141 15448406 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/zHELICOPTER.mp4\n",
      "Video 1 of 3\n",
      "Number of frames in the video: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/142 [00:00<?, ?frame/s]W0000 00:00:1749735315.860851 15522911 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.870455 15522911 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.871588 15522909 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.871622 15522918 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.871629 15522913 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.874856 15522913 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.875774 15522918 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735315.876420 15522909 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames: 100%|██████████| 142/142 [00:07<00:00, 19.31frame/s]\n",
      "I0000 00:00:1749735323.209812 15448406 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/zBICYCLE_FIETS.mp4\n",
      "Video 2 of 3\n",
      "Number of frames in the video: 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/131 [00:00<?, ?frame/s]W0000 00:00:1749735323.270132 15523022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.279173 15523027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.280258 15523027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.280313 15523024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.280335 15523025 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.283451 15523021 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.284698 15523025 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735323.285111 15523020 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames: 100%|██████████| 131/131 [00:06<00:00, 19.20frame/s]\n",
      "I0000 00:00:1749735330.069264 15448406 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ./input_videos/S12B.mp4\n",
      "Video 3 of 3\n",
      "Number of frames in the video: 17940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   0%|          | 0/17940 [00:00<?, ?frame/s]W0000 00:00:1749735330.128912 15523099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.138766 15523100 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.139852 15523098 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.139953 15523106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.139960 15523097 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.143225 15523098 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.144039 15523101 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749735330.144426 15523107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing frames:  16%|█▋        | 2931/17940 [02:46<14:13, 17.59frame/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[1;32m     91\u001b[0m        \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m capture\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     94\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Which PoseLandmark indices we want to skip because\n",
    "SKIP_POSE_IDS = {\n",
    "    mp_holistic.PoseLandmark.LEFT_WRIST,\n",
    "    mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "    mp_holistic.PoseLandmark.LEFT_PINKY,\n",
    "    mp_holistic.PoseLandmark.RIGHT_PINKY,\n",
    "    mp_holistic.PoseLandmark.LEFT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_INDEX,\n",
    "    mp_holistic.PoseLandmark.LEFT_THUMB,\n",
    "    mp_holistic.PoseLandmark.RIGHT_THUMB,\n",
    "    mp_holistic.PoseLandmark.LEFT_HIP,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HIP,\n",
    "    mp_holistic.PoseLandmark.LEFT_KNEE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_KNEE,\n",
    "    mp_holistic.PoseLandmark.LEFT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_ANKLE,\n",
    "    mp_holistic.PoseLandmark.LEFT_HEEL,\n",
    "    mp_holistic.PoseLandmark.RIGHT_HEEL,\n",
    "    mp_holistic.PoseLandmark.LEFT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.RIGHT_FOOT_INDEX,\n",
    "    mp_holistic.PoseLandmark.NOSE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE,\n",
    "    mp_holistic.PoseLandmark.LEFT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_INNER,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EYE_OUTER,\n",
    "    mp_holistic.PoseLandmark.LEFT_EAR,\n",
    "    mp_holistic.PoseLandmark.RIGHT_EAR,\n",
    "    mp_holistic.PoseLandmark.MOUTH_LEFT,\n",
    "    mp_holistic.PoseLandmark.MOUTH_RIGHT, \n",
    "}\n",
    "# Process videos\n",
    "for vidf in vfiles:\n",
    "    print(f\"Processing video: {vidf}\")\n",
    "    print(f\"Video {vfiles.index(vidf)+1} of {len(vfiles)}\")\n",
    "    \n",
    "    videoname = vidf\n",
    "    videoloc = videoname\n",
    "    capture = cv2.VideoCapture(videoloc)\n",
    "    # get the number of frames in the video\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    samplerate = capture.get(cv2.CAP_PROP_FPS)\n",
    "    # get the number of frames in the video\n",
    "    num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # get the number of frames in the video\n",
    "    print(f\"Number of frames in the video: {num_frames}\")\n",
    "    with mp_holistic.Holistic(static_image_mode=False,           # Video stream mode :contentReference[oaicite:7]{index=7}\n",
    "    model_complexity=1,                # Highest-accuracy pose model :contentReference[oaicite:8]{index=8}\n",
    "    refine_face_landmarks=False,        # Finer facial detail (iris, contours) :contentReference[oaicite:9]{index=9}\n",
    "    enable_segmentation=False,          # Person mask for effects :contentReference[oaicite:10]{index=10}\n",
    "    smooth_landmarks=True,             # Temporal smoothing to reduce jitter :contentReference[oaicite:11]{index=11}\n",
    "    min_detection_confidence=0.7,      # Filter weak detections :contentReference[oaicite:12]{index=12}\n",
    "    min_tracking_confidence=0.7        # Filter unstable tracks :contentReference[oaicite:13]{index=13}\n",
    "    ) as holistic:\n",
    "        all_kpts = []\n",
    "        for i in tqdm(range(num_frames), desc=\"Processing frames\", unit=\"frame\"):\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image)\n",
    "            h, w, _ = image.shape\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            image, kpts = draw_and_save_custom_landmarks(\n",
    "                    image,\n",
    "                    results,\n",
    "                    skip_pose_ids=SKIP_POSE_IDS,\n",
    "                    pose_point_radius=5,\n",
    "                    pose_point_color=(0,255,0),\n",
    "                    connect_hands_to_body=True,\n",
    "                    arm_connection_color=(255,0,0),       # red lines for the “arm” link\n",
    "                    arm_connection_thickness=2,\n",
    "                    draw=True\n",
    "                )\n",
    "            all_kpts.append(kpts)\n",
    "\n",
    "            cv2.imshow(\"merged_landmarks\", image)\n",
    "\n",
    "            if cv2.waitKey(1) == 27:\n",
    "               break\n",
    "            cv2.waitKey(1)\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    all_kpts = np.array(all_kpts)\n",
    "    # Save the keypoints as npy array\n",
    "    video_name = vidf.split('/')[-1].split('.')[0]\n",
    "    np.save(outtputf_ts + video_name+ '_all_kpts_17.npy', all_kpts)\n",
    "    # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
